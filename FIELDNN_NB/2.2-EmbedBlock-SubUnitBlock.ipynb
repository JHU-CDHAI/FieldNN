{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7588aa63-50a8-4f68-b40d-5e65e1783790",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/floydluo/Library/CloudStorage/GoogleDrive-jjluo@terpmail.umd.edu/My Drive/0-Research-Project/MedStar/MS_CODE/FieldNN\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f1850-8d92-40fd-ba69-6657f63ca703",
   "metadata": {},
   "source": [
    "# Precode: Batch_rfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e6de85-919d-4ffa-b1d6-0f286e9dc6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ProcData/TensorFolder/Task2YearXXX\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A1C@DT-DTDftGrn',\n",
       " 'A1C@V-A1CNumeDftGrn',\n",
       " 'Diag@DT-DTDftGrn',\n",
       " 'Diag@Value-DiagDftGrn',\n",
       " 'EC@BasicInfo-BasicDftGrn',\n",
       " 'EC@DT_min-DTDftGrn',\n",
       " 'P@age-AgeNumeDftGrn',\n",
       " 'P@basicInfo-basicInfoDftGrn',\n",
       " 'PN@DT-DTDftGrn',\n",
       " 'PNSect@SectName-PNSctNmDftGrn',\n",
       " 'PNSectSent@Sentence-Tk@TknzLLMGrn',\n",
       " 'Smoking@DT-DTDftGrn',\n",
       " 'Smoking@V-SmokingDftGrn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from recfldgrn.datapoint import load_df_data_from_folder\n",
    "from fieldnn.utils.layerfn import traverse, convert_relational_list_to_numpy\n",
    "\n",
    "###################### take this as given\n",
    "batch_PID_order = ['P1', 'P4', 'P5', 'P6']\n",
    "######################\n",
    "\n",
    "TaskTensor_folder = 'data/ProcData/TensorFolder/Task2YearXXX'\n",
    "print(TaskTensor_folder)\n",
    "\n",
    "l = sorted([i for i in os.listdir(TaskTensor_folder) if 'Grn' in i])\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "148b0bee-5e48-47e4-abb7-eef0f9638c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recfldgrn_list = [\n",
    "                  'A1C@V-A1CNumeDftGrn',\n",
    "                  'Diag@DT-DTDftGrn',\n",
    "                  'Diag@Value-DiagDftGrn',\n",
    "                  'PNSectSent@Sentence-Tk@TknzLLMGrn'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f98dd28e-0fb5-43b2-ae78-5b07f07b814a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ProcData/TensorFolder/Task2YearXXX/A1C@V-A1CNumeDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX/Diag@DT-DTDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX/Diag@Value-DiagDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX/PNSectSent@Sentence-Tk@TknzLLMGrn\n",
      "B-P-EC-A1C@V-A1CNumeDftGrn_wgt (4, 25, 1, 37)\n",
      "B-P-EC-Diag@DT-DTDftGrn_idx (4, 25, 22, 7)\n",
      "B-P-EC-Diag@Value-DiagDftGrn_idx (4, 25, 22, 3)\n",
      "B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLMGrn_idx (4, 25, 1, 14, 121, 11)\n"
     ]
    }
   ],
   "source": [
    "batch_rfg = {}\n",
    "\n",
    "for recfldgrn in recfldgrn_list:\n",
    "    \n",
    "    # (1) get tensor_folder\n",
    "    tensor_folder = os.path.join(TaskTensor_folder, recfldgrn)\n",
    "    print(tensor_folder)\n",
    "\n",
    "    # (2) get df_Pat and full_recfldgrn\n",
    "    df_Pat = load_df_data_from_folder(tensor_folder).set_index('PID')\n",
    "    full_recfldgrn = df_Pat.columns[0]\n",
    "    suffix = full_recfldgrn.split('_')[-1]\n",
    "    assert recfldgrn in full_recfldgrn\n",
    "\n",
    "    # (3) load batch: TODO: convert this to DataSet and DataLoader\n",
    "    df_batch = df_Pat.loc[batch_PID_order]\n",
    "\n",
    "    # (4) tensor batch as tensor_idx\n",
    "    new_full_recfldgrn = 'B-' + full_recfldgrn\n",
    "    values_list = df_batch[full_recfldgrn].to_list()\n",
    "    suffix = full_recfldgrn.split('_')[-1]\n",
    "    # print(suffix)\n",
    "    # print(new_full_recfldgrn)\n",
    "    D = convert_relational_list_to_numpy(values_list, new_full_recfldgrn, suffix)\n",
    "    tensor_idx = D[new_full_recfldgrn]\n",
    "    \n",
    "    batch_rfg[new_full_recfldgrn] = tensor_idx\n",
    "    \n",
    "for k, v in batch_rfg.items(): print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc781d1-edf2-4cd2-8c5e-6f68a4a49417",
   "metadata": {},
   "source": [
    "# get df_SubUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7209fc-8b11-4168-b5f6-5fc10d39a3d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-P-EC-A1C@V-A1CNumeDftGrn_wgt',\n",
       " 'B-P-EC-Diag@Value-DiagDftGrn_idx',\n",
       " 'B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLMGrn_idx']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_recfldgrn_list = ['B-P-EC-A1C@V-A1CNumeDftGrn_wgt',\n",
    "                       'B-P-EC-Diag@Value-DiagDftGrn_idx',\n",
    "                       'B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLMGrn_idx']\n",
    "\n",
    "full_recfldgrn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b113ed-5259-4f4e-b836-64eb2b7c4bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to fieldnn.dataflowfn.embedflowfn.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_EmbeddingBlock_SubUnit(full_recfldgrn_list, default_E_subunit_name = 'E'):\n",
    "    SubUnit_List = []\n",
    "    for input_name in full_recfldgrn_list:\n",
    "        d = {}\n",
    "        d['SubUnitName'] = default_E_subunit_name\n",
    "        \n",
    "        recfldgrn = '-'.join(input_name.split('-')[-2:])\n",
    "        output_layerid = len(input_name.split('-'))\n",
    "        output_name = input_name.split('Grn')[0]\n",
    "        \n",
    "        d['input_names'] = [input_name]\n",
    "        d['output_name'] = output_name\n",
    "        \n",
    "        d['output_layerid'] = output_layerid\n",
    "        \n",
    "        SubUnit_List.append(d)\n",
    "        \n",
    "    df_SubUnit = pd.DataFrame(SubUnit_List)\n",
    "    \n",
    "    return df_SubUnit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa4f990a-2b17-4b0f-8c44-d2fd957ca6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubUnitName</th>\n",
       "      <th>input_names</th>\n",
       "      <th>output_name</th>\n",
       "      <th>output_layerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-A1C@V-A1CNumeDftGrn_wgt]</td>\n",
       "      <td>B-P-EC-A1C@V-A1CNumeDft</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-Diag@Value-DiagDftGrn_idx]</td>\n",
       "      <td>B-P-EC-Diag@Value-DiagDft</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...</td>\n",
       "      <td>B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SubUnitName                                        input_names  \\\n",
       "0           E                   [B-P-EC-A1C@V-A1CNumeDftGrn_wgt]   \n",
       "1           E                 [B-P-EC-Diag@Value-DiagDftGrn_idx]   \n",
       "2           E  [B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...   \n",
       "\n",
       "                                       output_name  output_layerid  \n",
       "0                          B-P-EC-A1C@V-A1CNumeDft               5  \n",
       "1                        B-P-EC-Diag@Value-DiagDft               5  \n",
       "2  B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM               7  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_E_subunit_name = 'E'\n",
    "\n",
    "df_SubUnit = get_EmbeddingBlock_SubUnit(full_recfldgrn_list, default_E_subunit_name)\n",
    "df_SubUnit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b256f270-7138-44fc-b149-10c7f65487ab",
   "metadata": {},
   "source": [
    "# get df_SubUnit Extra Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca54d64-d838-466c-bd20-708ffd9d3d4c",
   "metadata": {},
   "source": [
    "## Get SubUnit Name to NN List\n",
    "\n",
    "* E --> [E]\n",
    "* MR --> [M, R]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "308036f5-cc2b-4be7-8ba7-1599992b4bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fieldnn.dataflowfn.baseflowfn import mapping_SubUnitName_to_SubUnitNNList\n",
    "\n",
    "def mapping_SubUnitName_to_SubUnitNNList(SubUnitName, input_names, \n",
    "                                         default_BasicNNtype_To_NNName):\n",
    "    SubUnitNNList = []\n",
    "    for idx, NAME in enumerate(SubUnitName):\n",
    "        if NAME == 'E':\n",
    "            nn_type = 'expander'\n",
    "            input_name = input_names[0]\n",
    "            \n",
    "            assert 'Grn' in input_name\n",
    "            assert len(input_names) == 1\n",
    "            \n",
    "            if 'Tknz' in input_name:\n",
    "                nn_name = 'LLMEmbed'\n",
    "            elif '_wgt' in input_name:\n",
    "                nn_name = 'NumeEmbed'\n",
    "            else:\n",
    "                nn_name = 'CateEmbed'\n",
    "            \n",
    "        elif NAME == 'R':\n",
    "            if idx == 0: assert len(input_names) == 1\n",
    "            nn_type = 'reducer'\n",
    "            nn_name = default_BasicNNtype_To_NNName[nn_type]\n",
    "            \n",
    "        elif NAME == 'M':\n",
    "            nn_type = 'merger'\n",
    "            nn_name = default_BasicNNtype_To_NNName[nn_type]\n",
    "            \n",
    "        elif NAME == 'L':\n",
    "            # print(input_names)\n",
    "            assert idx != 0\n",
    "            # assert len(input_names) == 1; not necessary, L can follow M\n",
    "            nn_type = 'learner'\n",
    "            input_name = input_names[0]\n",
    "            layers_num = len(input_name.split('-'))\n",
    "            if layers_num >= 3:\n",
    "                nn_name = 'TFM'\n",
    "            elif layers_num == 2:\n",
    "                nn_name = 'Linear'\n",
    "            else:\n",
    "                raise ValueError(f'incorrect layers_num {layers_num}')\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f'The BasicNN is not correct {NAME}')\n",
    "        \n",
    "        SubUnitNNList.append(nn_type + '-' + nn_name)\n",
    "    return SubUnitNNList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a3f2e77-67a5-41e2-b1e8-da4e306764c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################################# Hyperparameters\n",
    "default_BasicNNtype_To_NNName = {\n",
    "    'expander': None, # will be updated according to the Grn Type\n",
    "    'reducer': 'Max',\n",
    "    'merger': 'Merger',\n",
    "    'learner': None, # TODO: ignore this currently\n",
    "}\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f446db25-1fed-4722-a770-87545b0e449d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubUnitName</th>\n",
       "      <th>input_names</th>\n",
       "      <th>output_name</th>\n",
       "      <th>output_layerid</th>\n",
       "      <th>SubUnit_BasicNN_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-A1C@V-A1CNumeDftGrn_wgt]</td>\n",
       "      <td>B-P-EC-A1C@V-A1CNumeDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-NumeEmbed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-Diag@Value-DiagDftGrn_idx]</td>\n",
       "      <td>B-P-EC-Diag@Value-DiagDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...</td>\n",
       "      <td>B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM</td>\n",
       "      <td>7</td>\n",
       "      <td>[expander-LLMEmbed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SubUnitName                                        input_names  \\\n",
       "0           E                   [B-P-EC-A1C@V-A1CNumeDftGrn_wgt]   \n",
       "1           E                 [B-P-EC-Diag@Value-DiagDftGrn_idx]   \n",
       "2           E  [B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...   \n",
       "\n",
       "                                       output_name  output_layerid  \\\n",
       "0                          B-P-EC-A1C@V-A1CNumeDft               5   \n",
       "1                        B-P-EC-Diag@Value-DiagDft               5   \n",
       "2  B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM               7   \n",
       "\n",
       "   SubUnit_BasicNN_List  \n",
       "0  [expander-NumeEmbed]  \n",
       "1  [expander-CateEmbed]  \n",
       "2   [expander-LLMEmbed]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df_SubUnit.apply(lambda x: mapping_SubUnitName_to_SubUnitNNList(x['SubUnitName'], \n",
    "                                                                    x['input_names'],\n",
    "                                                                    default_BasicNNtype_To_NNName), \n",
    "                    axis = 1)\n",
    "\n",
    "\n",
    "df_SubUnit['SubUnit_BasicNN_List'] = s\n",
    "df_SubUnit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110c3ba-d710-4549-ab29-3d3a4c0d3eeb",
   "metadata": {},
   "source": [
    "## Get Default BasicNN Config List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6f35d7d-4e89-4881-88cf-31edb7ed32c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_Default_ExpanderNNPara(full_recfldgrn, fldgrn_folder):\n",
    "    \n",
    "    # (1) get basic information\n",
    "    recfld = [i for i in full_recfldgrn.split('-') if '@' in i][0]\n",
    "    rec, fld = recfld.split('@')\n",
    "    grn_suffix = [i for i in full_recfldgrn.split('-') if 'Grn' in i][0]\n",
    "    grn, suffix = grn_suffix.split('_')\n",
    "    prefix_ids = [i for i in full_recfldgrn.split('-') if 'Grn' not in i and '@' not in i]\n",
    "    recfldgrn = rec + '@' + fld + '-' + grn\n",
    "\n",
    "    # (2) get vocab information\n",
    "    # fldgrn_folder = 'data/ProcData/FldGrnInfo'\n",
    "    fullfldgrn_file = os.path.join(fldgrn_folder, rec + '.p')\n",
    "    df_FieldGrainInfo = pd.read_pickle(fullfldgrn_file)\n",
    "\n",
    "\n",
    "    # (3) get vocab information\n",
    "    # no matter what type of grain, in the end, we will have vocab_tokenizer.\n",
    "    if 'LLM' in full_recfldgrn:\n",
    "        vocab_tokenizer = df_FieldGrainInfo[df_FieldGrainInfo['recfield2grain'] == recfldgrn].iloc[0]['Vocab']['v2idx']\n",
    "        init = vocab_tokenizer.name_or_path\n",
    "    else:\n",
    "        vocab_tokenizer = df_FieldGrainInfo[df_FieldGrainInfo['recfield2grain'] == recfldgrn].iloc[0]['Vocab']['v2idx']\n",
    "        init = 'random'\n",
    "\n",
    "    d = {'vocab_tokenizer': vocab_tokenizer, 'init': init}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a6fda49-c3b3-4664-b2a6-4ce4a2d66566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_SubUnit_Default_NNPara_List(SubUnit_BasicNN_List, SubUnit_input_names, \n",
    "                                    fldgrn_folder, learner_default_dict):\n",
    "    \n",
    "    SubUnit_DefaultBasicNN_List = []\n",
    "    for basic_nn_idx, nn_type_nn_name in enumerate(SubUnit_BasicNN_List):\n",
    "        nn_type, nn_name = nn_type_nn_name.split('-')\n",
    "        \n",
    "        if basic_nn_idx == 0:\n",
    "            # the first BasicNN in the SubUnit\n",
    "            # expander and merger will only be here.\n",
    "            # this assigments only works for the first iteration\n",
    "            # let's make a village rule: only E, R, M can be the first. \n",
    "            assert nn_type in ['expander', 'reducer', 'merger']\n",
    "        \n",
    "            if nn_type == 'expander':\n",
    "                input_names_nnlvl = SubUnit_input_names \n",
    "                assert len(input_names_nnlvl) == 1\n",
    "                full_recfldgrn = input_names_nnlvl[0]\n",
    "                default_para = get_Default_ExpanderNNPara(full_recfldgrn, fldgrn_folder)\n",
    "                \n",
    "            else:\n",
    "                default_para = {}\n",
    "            \n",
    "        else:\n",
    "            assert nn_type in ['reducer', 'learner']\n",
    "            if nn_type == 'learner':\n",
    "                default_para = learner_default_dict[nn_name] # TODO\n",
    "            else:\n",
    "                default_para = {}\n",
    "        \n",
    "        SubUnit_DefaultBasicNN_List.append(default_para)    \n",
    "        \n",
    "    return SubUnit_DefaultBasicNN_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7318b03-899a-4528-8f07-22d5555e5155",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubUnitName</th>\n",
       "      <th>input_names</th>\n",
       "      <th>output_name</th>\n",
       "      <th>output_layerid</th>\n",
       "      <th>SubUnit_BasicNN_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-A1C@V-A1CNumeDftGrn_wgt]</td>\n",
       "      <td>B-P-EC-A1C@V-A1CNumeDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-NumeEmbed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-Diag@Value-DiagDftGrn_idx]</td>\n",
       "      <td>B-P-EC-Diag@Value-DiagDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...</td>\n",
       "      <td>B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM</td>\n",
       "      <td>7</td>\n",
       "      <td>[expander-LLMEmbed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SubUnitName                                        input_names  \\\n",
       "0           E                   [B-P-EC-A1C@V-A1CNumeDftGrn_wgt]   \n",
       "1           E                 [B-P-EC-Diag@Value-DiagDftGrn_idx]   \n",
       "2           E  [B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...   \n",
       "\n",
       "                                       output_name  output_layerid  \\\n",
       "0                          B-P-EC-A1C@V-A1CNumeDft               5   \n",
       "1                        B-P-EC-Diag@Value-DiagDft               5   \n",
       "2  B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM               7   \n",
       "\n",
       "   SubUnit_BasicNN_List  \n",
       "0  [expander-NumeEmbed]  \n",
       "1  [expander-CateEmbed]  \n",
       "2   [expander-LLMEmbed]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_SubUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "761a0361-9a4b-4035-9f1a-b35b0d26edd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SubUnitName                                            E\n",
       "input_names             [B-P-EC-A1C@V-A1CNumeDftGrn_wgt]\n",
       "output_name                      B-P-EC-A1C@V-A1CNumeDft\n",
       "output_layerid                                         5\n",
       "SubUnit_BasicNN_List                [expander-NumeEmbed]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SubUnit_info = df_SubUnit.iloc[0]\n",
    "SubUnit_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d356d74a-7630-47e3-acb1-08b10de14e67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SubUnit_BasicNN_List = SubUnit_info['SubUnit_BasicNN_List']\n",
    "SubUnit_input_names = SubUnit_info['input_names']\n",
    "SubUnit_output_name = SubUnit_info['output_name']\n",
    "fldgrn_folder = 'data/ProcData/FldGrnInfo'\n",
    "learner_default_dict = {} # To update it in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16da2c37-ac1f-43c9-a52c-3f971c41649f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['expander-NumeEmbed']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SubUnit_BasicNN_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "265c1706-59f0-4e08-b3a3-d4aba4d6ddc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['vocab_tokenizer', 'init'])\n"
     ]
    }
   ],
   "source": [
    "SubUnit_DefaultBasicNN_List = get_SubUnit_Default_NNPara_List(SubUnit_BasicNN_List, SubUnit_input_names, \n",
    "                                                              fldgrn_folder, learner_default_dict)\n",
    "\n",
    "for i in SubUnit_DefaultBasicNN_List:\n",
    "    print(i.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2802852-c921-40dd-947c-15f9b64a3162",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubUnitName</th>\n",
       "      <th>input_names</th>\n",
       "      <th>output_name</th>\n",
       "      <th>output_layerid</th>\n",
       "      <th>SubUnit_BasicNN_List</th>\n",
       "      <th>SubUnit_DefaultBasicNN_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-A1C@V-A1CNumeDftGrn_wgt]</td>\n",
       "      <td>B-P-EC-A1C@V-A1CNumeDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-NumeEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-Diag@Value-DiagDftGrn_idx]</td>\n",
       "      <td>B-P-EC-Diag@Value-DiagDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, '_missing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...</td>\n",
       "      <td>B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM</td>\n",
       "      <td>7</td>\n",
       "      <td>[expander-LLMEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': BertTokenizerFast(name_or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SubUnitName                                        input_names  \\\n",
       "0           E                   [B-P-EC-A1C@V-A1CNumeDftGrn_wgt]   \n",
       "1           E                 [B-P-EC-Diag@Value-DiagDftGrn_idx]   \n",
       "2           E  [B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...   \n",
       "\n",
       "                                       output_name  output_layerid  \\\n",
       "0                          B-P-EC-A1C@V-A1CNumeDft               5   \n",
       "1                        B-P-EC-Diag@Value-DiagDft               5   \n",
       "2  B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM               7   \n",
       "\n",
       "   SubUnit_BasicNN_List                        SubUnit_DefaultBasicNN_List  \n",
       "0  [expander-NumeEmbed]  [{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, '_missing...  \n",
       "2   [expander-LLMEmbed]  [{'vocab_tokenizer': BertTokenizerFast(name_or...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fldgrn_folder = 'data/ProcData/FldGrnInfo'\n",
    "learner_default_dict = {} # To update it in the future. \n",
    "\n",
    "\n",
    "s = df_SubUnit.apply(lambda x: get_SubUnit_Default_NNPara_List(x['SubUnit_BasicNN_List'], \n",
    "                                                               x['input_names'],\n",
    "                                                               fldgrn_folder, \n",
    "                                                               learner_default_dict), axis = 1)\n",
    "\n",
    "df_SubUnit['SubUnit_DefaultBasicNN_List'] = s\n",
    "\n",
    "\n",
    "df_SubUnit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5d2183-c267-4526-a6b5-c6ac6307391e",
   "metadata": {},
   "source": [
    "## Get BasicNN Config List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f291e204-ba6a-4ac3-8afa-0c00f6aff8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fieldnn.configfn.expanderfn import get_expander_para\n",
    "from fieldnn.configfn.mergerfn import get_merger_para   # ignore this one in this notebook\n",
    "from fieldnn.configfn.reducerfn import get_reducer_para # ignore this one in this notebook\n",
    "from fieldnn.configfn.learnerfn import get_learner_para # ignore this one in this notebook\n",
    "\n",
    "\n",
    "\n",
    "def generate_BasicNN_Config(nn_type_nn_name, \n",
    "                            input_names_nnlvl, \n",
    "                            default_nnpara, \n",
    "                            embed_size, \n",
    "                            process):\n",
    "    '''\n",
    "        please notince here, this function is not the final version yet.\n",
    "    '''\n",
    "    nn_type, nn_name = nn_type_nn_name.split('-')\n",
    "\n",
    "    if nn_type == 'expander':\n",
    "        \n",
    "        assert len(input_names_nnlvl) == 1\n",
    "        fld = input_names_nnlvl[0].split('-')[-1]\n",
    "        # Get the output_name_nnlvl\n",
    "        output_name_nnlvl = input_names_nnlvl[0].split('Grn')[0]\n",
    "        \n",
    "        # Get the input_size and output_size\n",
    "        input_size = None\n",
    "        output_size = embed_size \n",
    "        \n",
    "        # Get the postprocess\n",
    "        postprocess = process\n",
    "        \n",
    "        # Derive the para\n",
    "        vocab_tokenizer = default_nnpara['vocab_tokenizer']\n",
    "        init = default_nnpara['init']\n",
    "        para = get_expander_para(nn_name, default_nnpara, \n",
    "                                 embed_size, vocab_tokenizer, init, postprocess)\n",
    "\n",
    "    elif nn_type == 'reducer': \n",
    "        \n",
    "        assert len(input_names_nnlvl) == 1\n",
    "        fld = input_names_nnlvl[0].split('-')[-1]\n",
    "        # Get the output_name_nnlvl\n",
    "        output_name_nnlvl = input_names_nnlvl[0].replace('-' + fld, '@' + fld)\n",
    "\n",
    "        # Get the para for the NN layer\n",
    "        nn_para = default_nnpara # this will be updated.\n",
    "\n",
    "        # Get the input_size\n",
    "        input_size = embed_size\n",
    "\n",
    "        # Get the output_size\n",
    "        if nn_name.lower() == 'concat':\n",
    "            # this types of merger only happened in the R in MLRL. \n",
    "            # you can skip this part safely if you haven't encountered M. \n",
    "            fld_childflds = input_names_nnlvl[0].split('-')[-1]\n",
    "            assert '@' in fld_childflds\n",
    "            childflds = fld_childflds.split('@')[-1]\n",
    "            childflds = childflds.split('&')\n",
    "            output_size = len(childflds) * embed_size\n",
    "        else:\n",
    "            # usually the most case. \n",
    "            output_size = embed_size \n",
    "\n",
    "        # Get the postprocess\n",
    "        postprocess = process\n",
    "\n",
    "        # Derive the para\n",
    "        para = get_reducer_para(nn_name, default_nnpara, input_size, output_size, postprocess)  \n",
    "        \n",
    "    elif nn_type == 'merger':\n",
    "        # generate the output name\n",
    "        \n",
    "        assert len(input_names_nnlvl) > 1\n",
    "        # input_names_nnlvl: ['B-P-EC-A1C@DT', 'B-P-EC-A1C@V']\n",
    "        \n",
    "        childflds = [i.split('-')[-1] for i in input_names_nnlvl]\n",
    "        # childflds: ['A1C@DT', 'A1C@V']\n",
    "        \n",
    "        fld_childflds = '&'.join([i.split('@')[-1] for i in childflds])\n",
    "        # fld_childflds: DT&V\n",
    "        \n",
    "        output_prefix = input_names_nnlvl[0].replace('@' + childflds[0].split('@')[-1], '')\n",
    "        # output_prefix: B-P-EC-A1C\n",
    "        \n",
    "        output_name_nnlvl = output_prefix + '-' + fld_childflds\n",
    "        # output_name_nnlvl: B-P-EC-A1C-DT&V\n",
    "        \n",
    "        # Prepare the para for the NN layer\n",
    "        nn_para = default_nnpara # this will be updated.\n",
    "\n",
    "        # Get the input_size\n",
    "        input_size = embed_size\n",
    "        output_size = embed_size\n",
    "\n",
    "        # Get the postprocess\n",
    "        postprocess = process\n",
    "\n",
    "        # Derive the para\n",
    "        para = get_merger_para(nn_name, default_nnpara, input_size, output_size, postprocess) \n",
    "        \n",
    "        \n",
    "    elif nn_type == 'learner':\n",
    "        # generate the output name\n",
    "        assert len(input_names_nnlvl) == 1\n",
    "        output_name_nnlvl = input_names_nnlvl[0] # just the same name as before\n",
    "        \n",
    "        # Prepare the para for the NN layer\n",
    "        nn_para = default_nnpara # this will be updated.\n",
    "\n",
    "        # Get the input_size\n",
    "        input_size = embed_size\n",
    "        output_size = embed_size\n",
    "\n",
    "        # Get the postprocess\n",
    "        postprocess = process\n",
    "\n",
    "        # Derive the para\n",
    "        para = get_learner_para(nn_name, default_nnpara, input_size, output_size, postprocess)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f'nn_type {nn_type} is not available yet')\n",
    "\n",
    "    Basic_Config = {'input_names_nnlvl': input_names_nnlvl, \n",
    "                    'output_name_nnlvl': output_name_nnlvl, \n",
    "                    f'{nn_type}_para': para}\n",
    "    \n",
    "    return Basic_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b126004-41d6-4890-bea8-c86e37266441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_SubUnit_BasicNN_Config_List(SubUnit_BasicNN_List, \n",
    "                                    SubUnit_DefaultBasicNN_List, \n",
    "                                    SubUnit_input_names, \n",
    "                                    SubUnit_output_name, \n",
    "                                    embed_size, \n",
    "                                    process, \n",
    "                                   ):\n",
    "    \n",
    "    \n",
    "    # TODO: also add the layer_idx in order to deal with the learn_layer_para.\n",
    "    # This function also needs to be updated. \n",
    "    \n",
    "    SubUnit_BasicNN_Config_List = []\n",
    "    \n",
    "    # print('\\n\\n************** SubUnit_BasicNN_List ****************')\n",
    "    # print(SubUnit_BasicNN_List)\n",
    "    # print(SubUnit_input_names)\n",
    "    # print(SubUnit_output_name)\n",
    "    for basic_nn_idx, nn_type_nn_name in enumerate(SubUnit_BasicNN_List):\n",
    "\n",
    "        # Get the input_names_nnlvl\n",
    "        if basic_nn_idx == 0:\n",
    "            input_names_nnlvl = SubUnit_input_names # this assigments only works for the first iteration\n",
    "        else:\n",
    "            input_names_nnlvl = [output_name_nnlvl]\n",
    "        \n",
    "        ##############\n",
    "        default_nnpara = SubUnit_DefaultBasicNN_List[basic_nn_idx] \n",
    "        ##############\n",
    "        \n",
    "        \n",
    "        Basic_Config = generate_BasicNN_Config(nn_type_nn_name, \n",
    "                                               input_names_nnlvl, \n",
    "                                               default_nnpara, \n",
    "                                               embed_size, \n",
    "                                               process)\n",
    "        output_name_nnlvl = Basic_Config['output_name_nnlvl']\n",
    "        BasicNN_Config = {'nn_type_nn_name': nn_type_nn_name, 'Basic_Config': Basic_Config}\n",
    "        SubUnit_BasicNN_Config_List.append(BasicNN_Config)\n",
    "        \n",
    "        # print('==========================')\n",
    "        # print(basic_nn_idx, nn_type_nn_name)\n",
    "        # print(input_names_nnlvl, '<-------- input_names_nnlvl')\n",
    "        # print(output_name_nnlvl, '<-------- output_name_nnlvl')\n",
    "        \n",
    "        \n",
    "        # also check the input_size of dim and output_size of dim\n",
    "\n",
    "    final_output_name_nnlvl = output_name_nnlvl\n",
    "\n",
    "    # if not SubUnit_output_name in final_output_name_nnlvl:\n",
    "    #     print('xxx errors xxx')\n",
    "    #     print(final_output_name_nnlvl, '<------- final_output_name_nnlvl')\n",
    "    #     print(SubUnit_output_name, '<------- SubUnit_output_name')\n",
    "    #     print('xxx errors xxx')\n",
    "    assert SubUnit_output_name in final_output_name_nnlvl\n",
    "    # print('\\n************** End SubUnit_BasicNN_List ****************')\n",
    "    return SubUnit_BasicNN_Config_List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "348c8c6e-96ac-4c3e-9992-34197a4154df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SubUnit_BasicNN_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3de3f837-643b-468c-ac36-f6a81d1302ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SubUnit_DefaultBasicNN_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0590a4cf-d0cb-4159-a04d-8b2d14db09ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################\n",
    "embed_size = 128\n",
    "process = {'activator': 'gelu',\n",
    "           'dropout': {'p': 0.5, 'inplace': False},\n",
    "           'layernorm': {'eps': 1e-05, 'elementwise_affine': True}}\n",
    "############################\n",
    "\n",
    "s = get_SubUnit_BasicNN_Config_List(SubUnit_BasicNN_List, \n",
    "                                    SubUnit_DefaultBasicNN_List, \n",
    "                                    SubUnit_input_names, \n",
    "                                    SubUnit_output_name, \n",
    "                                    embed_size, \n",
    "                                    process, \n",
    "                                   )\n",
    "\n",
    "SubUnit_BasicNN_Config_List = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bbffb436-3428-4c4e-a669-6029624cf84b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nn_type_nn_name': 'expander-NumeEmbed',\n",
       "  'Basic_Config': {'input_names_nnlvl': ['B-P-EC-A1C@V-A1CNumeDftGrn_wgt'],\n",
       "   'output_name_nnlvl': 'B-P-EC-A1C@V-A1CNumeDft',\n",
       "   'expander_para': {'nn_type': 'expander',\n",
       "    'nn_name': 'NumeEmbed',\n",
       "    'nn_para': {'embedding_size': 128, 'init': 'random', 'vocab_size': 37},\n",
       "    'input_size': None,\n",
       "    'output_size': 128,\n",
       "    'postprocess': {'activator': 'gelu',\n",
       "     'dropout': {'p': 0.5, 'inplace': False},\n",
       "     'layernorm': {'eps': 1e-05, 'elementwise_affine': True}}}}}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SubUnit_BasicNN_Config_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a8cef11-12bc-4848-ab15-cfa7045798cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubUnitName</th>\n",
       "      <th>input_names</th>\n",
       "      <th>output_name</th>\n",
       "      <th>output_layerid</th>\n",
       "      <th>SubUnit_BasicNN_List</th>\n",
       "      <th>SubUnit_DefaultBasicNN_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-A1C@V-A1CNumeDftGrn_wgt]</td>\n",
       "      <td>B-P-EC-A1C@V-A1CNumeDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-NumeEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-Diag@Value-DiagDftGrn_idx]</td>\n",
       "      <td>B-P-EC-Diag@Value-DiagDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, '_missing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...</td>\n",
       "      <td>B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM</td>\n",
       "      <td>7</td>\n",
       "      <td>[expander-LLMEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': BertTokenizerFast(name_or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SubUnitName                                        input_names  \\\n",
       "0           E                   [B-P-EC-A1C@V-A1CNumeDftGrn_wgt]   \n",
       "1           E                 [B-P-EC-Diag@Value-DiagDftGrn_idx]   \n",
       "2           E  [B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...   \n",
       "\n",
       "                                       output_name  output_layerid  \\\n",
       "0                          B-P-EC-A1C@V-A1CNumeDft               5   \n",
       "1                        B-P-EC-Diag@Value-DiagDft               5   \n",
       "2  B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM               7   \n",
       "\n",
       "   SubUnit_BasicNN_List                        SubUnit_DefaultBasicNN_List  \n",
       "0  [expander-NumeEmbed]  [{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, '_missing...  \n",
       "2   [expander-LLMEmbed]  [{'vocab_tokenizer': BertTokenizerFast(name_or...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_SubUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69afd91b-0a1c-49d5-a123-b431f758850c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################\n",
    "embed_size = 128\n",
    "process = {'activator': 'gelu',\n",
    "           'dropout': {'p': 0.5, 'inplace': False},\n",
    "           'layernorm': {'eps': 1e-05, 'elementwise_affine': True}}\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2202db12-e84e-458e-afac-c658fdb7d8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubUnitName</th>\n",
       "      <th>input_names</th>\n",
       "      <th>output_name</th>\n",
       "      <th>output_layerid</th>\n",
       "      <th>SubUnit_BasicNN_List</th>\n",
       "      <th>SubUnit_DefaultBasicNN_List</th>\n",
       "      <th>SubUnit_BasicNN_Config_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-A1C@V-A1CNumeDftGrn_wgt]</td>\n",
       "      <td>B-P-EC-A1C@V-A1CNumeDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-NumeEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-NumeEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-Diag@Value-DiagDftGrn_idx]</td>\n",
       "      <td>B-P-EC-Diag@Value-DiagDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, '_missing...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...</td>\n",
       "      <td>B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM</td>\n",
       "      <td>7</td>\n",
       "      <td>[expander-LLMEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': BertTokenizerFast(name_or...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-LLMEmbed', 'Bas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SubUnitName                                        input_names  \\\n",
       "0           E                   [B-P-EC-A1C@V-A1CNumeDftGrn_wgt]   \n",
       "1           E                 [B-P-EC-Diag@Value-DiagDftGrn_idx]   \n",
       "2           E  [B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...   \n",
       "\n",
       "                                       output_name  output_layerid  \\\n",
       "0                          B-P-EC-A1C@V-A1CNumeDft               5   \n",
       "1                        B-P-EC-Diag@Value-DiagDft               5   \n",
       "2  B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM               7   \n",
       "\n",
       "   SubUnit_BasicNN_List                        SubUnit_DefaultBasicNN_List  \\\n",
       "0  [expander-NumeEmbed]  [{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, '_missing...   \n",
       "2   [expander-LLMEmbed]  [{'vocab_tokenizer': BertTokenizerFast(name_or...   \n",
       "\n",
       "                         SubUnit_BasicNN_Config_List  \n",
       "0  [{'nn_type_nn_name': 'expander-NumeEmbed', 'Ba...  \n",
       "1  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "2  [{'nn_type_nn_name': 'expander-LLMEmbed', 'Bas...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df_SubUnit.apply(lambda x: get_SubUnit_BasicNN_Config_List(x['SubUnit_BasicNN_List'], \n",
    "                                                               x['SubUnit_DefaultBasicNN_List'], \n",
    "                                                               x['input_names'], \n",
    "                                                               x['output_name'], \n",
    "                                                                embed_size, \n",
    "                                                                process, \n",
    "                                                               ), axis = 1)\n",
    "\n",
    "s\n",
    "\n",
    "df_SubUnit['SubUnit_BasicNN_Config_List'] = s\n",
    "df_SubUnit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d263698-1555-413c-9730-e42ebba649c2",
   "metadata": {},
   "source": [
    "# SubUnit \n",
    "\n",
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3398cd46-f65c-4004-b3f9-ee429252807a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from fieldnn.basicnn.expander import Expander_Layer\n",
    "from fieldnn.basicnn.reducer import Reducer_Layer\n",
    "from fieldnn.basicnn.merger import Merger_Layer\n",
    "from fieldnn.basicnn.learner import Learner_Layer\n",
    "\n",
    "# from ..basicnn.expander import Expander_Layer\n",
    "# from ..basicnn.reducer import Reducer_Layer\n",
    "# from ..basicnn.merger import Merger_Layer\n",
    "# from ..basicnn.learner import Learner_Layer\n",
    "\n",
    "\n",
    "class SubUnit_Layer(torch.nn.Module):\n",
    "    '''Currently, it is not latest version'''\n",
    "    \n",
    "    def __init__(self, SubUnit_info):\n",
    "        super(SubUnit_Layer, self).__init__()\n",
    "        \n",
    "        # the input names for the SubUnit\n",
    "        self.SubUnit_input_names = SubUnit_info['input_names']\n",
    "        \n",
    "        # the output name for this SubUnit\n",
    "        self.SubUnit_output_name = SubUnit_info['output_name']\n",
    "        \n",
    "        # get the SubUnit's BasicNN Config List\n",
    "        self.SubUnit_BasicNN_Config_List = SubUnit_info['SubUnit_BasicNN_Config_List']\n",
    "        \n",
    "        \n",
    "        # construct the LayersDict to hold all BasicNN within this SubUnit.\n",
    "        self.LayersDict = torch.nn.ModuleDict()\n",
    "        \n",
    "        # initialize all the BasicNN for this SubUnit.\n",
    "        for idx, BasicNN_Config_Dict in enumerate(self.SubUnit_BasicNN_Config_List):\n",
    "            \n",
    "            # nn_type_nn_name: like reducer-Max, learner-TFM, merger-Merger, expander-llmembed.\n",
    "            nn_type_nn_name = BasicNN_Config_Dict['nn_type_nn_name']\n",
    "            \n",
    "            # Basic_Config: the Config for this NN.\n",
    "            Basic_Config = BasicNN_Config_Dict['Basic_Config']\n",
    "            \n",
    "            input_names_nnlvl = Basic_Config['input_names_nnlvl']\n",
    "            output_name_nnlvl = Basic_Config['output_name_nnlvl']\n",
    "                \n",
    "            if 'expander' in nn_type_nn_name:\n",
    "                expander_para = Basic_Config['expander_para']\n",
    "                NN = Expander_Layer(input_names_nnlvl, output_name_nnlvl, expander_para)\n",
    "                self.LayersDict[f'{idx}_{nn_type_nn_name}'] = NN\n",
    "                \n",
    "            elif 'reducer' in nn_type_nn_name:\n",
    "                reducer_para = Basic_Config['reducer_para']\n",
    "                NN = Reducer_Layer(input_names_nnlvl, output_name_nnlvl, reducer_para)\n",
    "                self.LayersDict[f'{idx}_{nn_type_nn_name}'] = NN\n",
    "                \n",
    "            elif 'merger' in nn_type_nn_name:\n",
    "                merger_para = Basic_Config['merger_para']\n",
    "                NN = Merger_Layer(input_names_nnlvl, output_name_nnlvl, merger_para)\n",
    "                self.LayersDict[f'{idx}_{nn_type_nn_name}'] = NN\n",
    "                \n",
    "                \n",
    "            elif 'learner' in nn_type_nn_name:\n",
    "                learner_para = Basic_Config['learner_para']\n",
    "                NN = Learner_Layer(input_names_nnlvl, output_name_nnlvl, learner_para)\n",
    "                self.LayersDict[f'{idx}_{nn_type_nn_name}'] = NN\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f'Current BasicNN {nn_type_nn_name} is not available')\n",
    "\n",
    "\n",
    "    def forward(self, SubUnit_input_names, RECFLD_TO_TENSOR):\n",
    "\n",
    "        INPUTS_TO_INFODICT = {}\n",
    "        \n",
    "        for idx, BasicNN_Config_Dict in enumerate(self.SubUnit_BasicNN_Config_List):\n",
    "            \n",
    "            nn_type_nn_name = BasicNN_Config_Dict['nn_type_nn_name']\n",
    "            Basic_Config = BasicNN_Config_Dict['Basic_Config']\n",
    "            input_names_nnlvl = Basic_Config['input_names_nnlvl']\n",
    "            output_name_nnlvl = Basic_Config['output_name_nnlvl']\n",
    "            \n",
    "            NN = self.LayersDict[f'{idx}_{nn_type_nn_name}']\n",
    "            \n",
    "            # prepare the input.\n",
    "            if idx == 0:\n",
    "                for tensor_name in input_names_nnlvl:\n",
    "                    INPUTS_TO_INFODICT[tensor_name] = RECFLD_TO_TENSOR[tensor_name]\n",
    "            else:\n",
    "                for input_name in input_names_nnlvl:\n",
    "                    assert input_name in INPUTS_TO_INFODICT\n",
    "                \n",
    "            output_name_nnlvl, info_dict = NN(input_names_nnlvl, INPUTS_TO_INFODICT)\n",
    "            \n",
    "            # current output will be the input in the next round. \n",
    "            INPUTS_TO_INFODICT[output_name_nnlvl] = info_dict\n",
    "            \n",
    "        # pick up the SubUnit_output_name and its info_dict\n",
    "        final_output_name_nnlvl = output_name_nnlvl\n",
    "        SubUnit_output_name = self.SubUnit_output_name\n",
    "        if not SubUnit_output_name in final_output_name_nnlvl:\n",
    "            print(f'Mismatched SubUnit Output and Final NN Output: {final_output_name_nnlvl} vs {output_name_nnlvl}')\n",
    "        \n",
    "        info_dict = INPUTS_TO_INFODICT[final_output_name_nnlvl]\n",
    "        \n",
    "        return SubUnit_output_name, info_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcef192-7ebe-49aa-9b92-b0fdb517fe57",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ed1122c-f8f4-4551-8545-2bf6988c016a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "OutputTensor_2_Embed = {}\n",
    "\n",
    "for idx, SubUnit_info in df_SubUnit.iterrows():\n",
    "    # print(SubUnit_info)\n",
    "    \n",
    "    output_name = SubUnit_info['output_name']\n",
    "    SubUnitLayer = SubUnit_Layer(SubUnit_info)\n",
    "    OutputTensor_2_Embed[output_name] = SubUnitLayer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48f16bab-c047-4785-b8ae-aa49b2fcc0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-P-EC-A1C@V-A1CNumeDftGrn_wgt',\n",
       " 'B-P-EC-Diag@DT-DTDftGrn_idx',\n",
       " 'B-P-EC-Diag@Value-DiagDftGrn_idx',\n",
       " 'B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLMGrn_idx']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in batch_rfg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54c25a1c-7c6f-4714-a34e-1e296ea1b771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RECFLD_TO_TENSOR = {}\n",
    "\n",
    "for full_recfldgrn in full_recfldgrn_list:\n",
    "    # (1) get the info_raw from batch_rfg\n",
    "    info_raw = batch_rfg[full_recfldgrn]\n",
    "   \n",
    "    # (2) get the holder (input_idx) and holder_wgt (for nume embedding only)\n",
    "    if '_idx' in full_recfldgrn:\n",
    "        holder_wgt = 'Empty'\n",
    "        holder = torch.LongTensor(info_raw)\n",
    "    elif '_wgt' in full_recfldgrn:\n",
    "        holder_wgt = torch.FloatTensor(info_raw)\n",
    "        # ATTENTION: here holder_wgt could contain zeros in some valid positions.\n",
    "        holder = torch.ones_like(holder_wgt).cumsum(-1).masked_fill(holder_wgt == 0, 0).long()\n",
    "    else:\n",
    "        raise ValueError(f'Invalid suffix \"{suffix}\"')\n",
    "\n",
    "    info_dict = {'holder': holder, 'holder_wgt': holder_wgt}\n",
    "    \n",
    "    RECFLD_TO_TENSOR[full_recfldgrn] = info_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8cdca40-6df6-4496-b07b-00d91217cb97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-P-EC-A1C@V-A1CNumeDft\n",
      "B-P-EC-Diag@Value-DiagDft\n",
      "B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_TO_TENSOR = {}\n",
    "\n",
    "for output_name, SubUnitLayer in OutputTensor_2_Embed.items():\n",
    "    input_names = SubUnitLayer.SubUnit_input_names\n",
    "    SubUnit_output_name, info_dict = SubUnitLayer(input_names, RECFLD_TO_TENSOR)\n",
    "    \n",
    "    print(SubUnit_output_name)\n",
    "    OUTPUT_TO_TENSOR[SubUnit_output_name] = info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba52978a-475b-4294-a87e-a29e73ce60c6",
   "metadata": {},
   "source": [
    "# EmbedBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ace42fd-6140-4c93-81a5-976c75b0635a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubUnitName</th>\n",
       "      <th>input_names</th>\n",
       "      <th>output_name</th>\n",
       "      <th>output_layerid</th>\n",
       "      <th>SubUnit_BasicNN_List</th>\n",
       "      <th>SubUnit_DefaultBasicNN_List</th>\n",
       "      <th>SubUnit_BasicNN_Config_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-A1C@V-A1CNumeDftGrn_wgt]</td>\n",
       "      <td>B-P-EC-A1C@V-A1CNumeDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-NumeEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-NumeEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-Diag@Value-DiagDftGrn_idx]</td>\n",
       "      <td>B-P-EC-Diag@Value-DiagDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, '_missing...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...</td>\n",
       "      <td>B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM</td>\n",
       "      <td>7</td>\n",
       "      <td>[expander-LLMEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': BertTokenizerFast(name_or...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-LLMEmbed', 'Bas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SubUnitName                                        input_names  \\\n",
       "0           E                   [B-P-EC-A1C@V-A1CNumeDftGrn_wgt]   \n",
       "1           E                 [B-P-EC-Diag@Value-DiagDftGrn_idx]   \n",
       "2           E  [B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...   \n",
       "\n",
       "                                       output_name  output_layerid  \\\n",
       "0                          B-P-EC-A1C@V-A1CNumeDft               5   \n",
       "1                        B-P-EC-Diag@Value-DiagDft               5   \n",
       "2  B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM               7   \n",
       "\n",
       "   SubUnit_BasicNN_List                        SubUnit_DefaultBasicNN_List  \\\n",
       "0  [expander-NumeEmbed]  [{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, '_missing...   \n",
       "2   [expander-LLMEmbed]  [{'vocab_tokenizer': BertTokenizerFast(name_or...   \n",
       "\n",
       "                         SubUnit_BasicNN_Config_List  \n",
       "0  [{'nn_type_nn_name': 'expander-NumeEmbed', 'Ba...  \n",
       "1  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "2  [{'nn_type_nn_name': 'expander-LLMEmbed', 'Bas...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eventually, we get a df_SubUnit\n",
    "df_SubUnit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac0791-6020-4cfa-b5b3-186ae2bb2bb9",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c085c0e-22c4-46e7-b1ea-d51b2a89eb76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# from .subunit import SubUnit_Layer\n",
    "\n",
    "\n",
    "class EmbedBlockLayer(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, df_SubUnit):\n",
    "        super(EmbedBlockLayer, self).__init__()\n",
    "        self.df_SubUnit = df_SubUnit\n",
    "        self.SubUnitDict = torch.nn.ModuleDict()\n",
    "        \n",
    "        for idx, SubUnit_info in df_SubUnit.iterrows():\n",
    "            output_name = SubUnit_info['output_name']\n",
    "            SubUnitLayer = SubUnit_Layer(SubUnit_info)\n",
    "            self.SubUnitDict[output_name] = SubUnitLayer\n",
    "\n",
    "    def forward(self, RECFLD_TO_TENSOR):\n",
    "        \n",
    "        OUTPUT_TO_TENSOR = {}\n",
    "        \n",
    "        for output_name, SubUnitLayer in self.SubUnitDict.items():\n",
    "            input_names = SubUnitLayer.SubUnit_input_names\n",
    "            SubUnit_output_name, info_dict = SubUnitLayer(input_names, RECFLD_TO_TENSOR)\n",
    "            \n",
    "            assert output_name == SubUnit_output_name\n",
    "            OUTPUT_TO_TENSOR[SubUnit_output_name] = info_dict\n",
    "        \n",
    "        return OUTPUT_TO_TENSOR\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac9fd2-177f-4508-b706-bf2267758ba6",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5464ff4a-aa9d-432d-990a-12776c5ba84f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmbedBlockLayer(\n",
       "  (SubUnitDict): ModuleDict(\n",
       "    (B-P-EC-A1C@V-A1CNumeDft): SubUnit_Layer(\n",
       "      (LayersDict): ModuleDict(\n",
       "        (0_expander-NumeEmbed): Expander_Layer(\n",
       "          (Embed): NumeEmbeddingLayer(\n",
       "            (embedding): Embedding(37, 128, padding_idx=0)\n",
       "          )\n",
       "          (postprocess): ModuleDict(\n",
       "            (activator): GELU(approximate='none')\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (B-P-EC-Diag@Value-DiagDft): SubUnit_Layer(\n",
       "      (LayersDict): ModuleDict(\n",
       "        (0_expander-CateEmbed): Expander_Layer(\n",
       "          (Embed): CateEmbeddingLayer(\n",
       "            (embedding): Embedding(801, 128, padding_idx=0)\n",
       "          )\n",
       "          (postprocess): ModuleDict(\n",
       "            (activator): GELU(approximate='none')\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM): SubUnit_Layer(\n",
       "      (LayersDict): ModuleDict(\n",
       "        (0_expander-LLMEmbed): Expander_Layer(\n",
       "          (Embed): LLMEmbeddingLayer(\n",
       "            (LLM): BertModel(\n",
       "              (embeddings): BertEmbeddings(\n",
       "                (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "                (position_embeddings): Embedding(512, 768)\n",
       "                (token_type_embeddings): Embedding(2, 768)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (encoder): BertEncoder(\n",
       "                (layer): ModuleList(\n",
       "                  (0-11): 12 x BertLayer(\n",
       "                    (attention): BertAttention(\n",
       "                      (self): BertSelfAttention(\n",
       "                        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (output): BertSelfOutput(\n",
       "                        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (intermediate): BertIntermediate(\n",
       "                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): BertOutput(\n",
       "                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (pooler): BertPooler(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (activation): Tanh()\n",
       "              )\n",
       "            )\n",
       "            (linear): Linear(in_features=768, out_features=128, bias=True)\n",
       "          )\n",
       "          (postprocess): ModuleDict(\n",
       "            (activator): GELU(approximate='none')\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "            (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EmbedBlock = EmbedBlockLayer(df_SubUnit)\n",
    "\n",
    "EmbedBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9432e540-2979-48fd-bce3-10ea5852f839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RECFLD_TO_TENSOR = {}\n",
    "\n",
    "for full_recfldgrn in full_recfldgrn_list:\n",
    "    # (1) get the info_raw from batch_rfg\n",
    "    info_raw = batch_rfg[full_recfldgrn]\n",
    "   \n",
    "    # (2) get the holder (input_idx) and holder_wgt (for nume embedding only)\n",
    "    if '_idx' in full_recfldgrn:\n",
    "        holder_wgt = 'Empty'\n",
    "        holder = torch.LongTensor(info_raw)\n",
    "    elif '_wgt' in full_recfldgrn:\n",
    "        holder_wgt = torch.FloatTensor(info_raw)\n",
    "        # ATTENTION: here holder_wgt could contain zeros in some valid positions.\n",
    "        holder = torch.ones_like(holder_wgt).cumsum(-1).masked_fill(holder_wgt == 0, 0).long()\n",
    "    else:\n",
    "        raise ValueError(f'Invalid suffix \"{suffix}\"')\n",
    "\n",
    "    info_dict = {'holder': holder, 'holder_wgt': holder_wgt}\n",
    "    \n",
    "    RECFLD_TO_TENSOR[full_recfldgrn] = info_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ff6e7f1-3c84-4523-a9cb-6e1a5b050eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_TO_TENSOR = EmbedBlock(RECFLD_TO_TENSOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de09c1fe-5907-4d66-b185-573f09ec770d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-P-EC-A1C@V-A1CNumeDft torch.Size([4, 25, 1, 37, 128])\n",
      "B-P-EC-Diag@Value-DiagDft torch.Size([4, 25, 22, 3, 128])\n",
      "B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM torch.Size([4, 25, 1, 14, 121, 11, 128])\n"
     ]
    }
   ],
   "source": [
    "for k, v in OUTPUT_TO_TENSOR.items():\n",
    "    print(k, v['info'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a3376c-b956-4743-ad56-5515d5b81493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908682b-9eee-494f-8680-62e471de209c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
