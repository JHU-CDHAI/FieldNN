{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85de7ae-af36-4d22-8e78-6d53cf43ec6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\My Drive\\0-Research-Project\\MedStar\\MS_CODE\\FieldNN\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6856af-fdcb-4d89-b714-e3774aed09c8",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "This is the for \n",
    "\n",
    "* module `fieldnn.basicnn.reducer` module\n",
    "* module `fieldnn.configfn.reducerfn` module\n",
    "* module `fieldnn.basicnn.merger` module\n",
    "* module `fieldnn.configfn.mergerfn` module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb79ef-c5ce-423d-bc4b-73b4cab4e43c",
   "metadata": {},
   "source": [
    "# PreCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66eb4992-cf60-4626-92df-e23c1f7890f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ProcData/TensorFolder/Task2YearXXX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from recfldgrn.datapoint import load_df_data_from_folder\n",
    "from fieldnn.utils.layerfn import traverse, convert_relational_list_to_numpy\n",
    "\n",
    "###################### take this as given\n",
    "batch_PID_order = ['P1', 'P4', 'P5', 'P6']\n",
    "######################\n",
    "\n",
    "TaskTensor_folder = 'data/ProcData/TensorFolder/Task2YearXXX'\n",
    "print(TaskTensor_folder)\n",
    "\n",
    "l = sorted([i for i in os.listdir(TaskTensor_folder) if 'Grn' in i])\n",
    "# l\n",
    "\n",
    "recfldgrn_list =['P@age-AgeNumeDftGrn',\n",
    "                 'P@basicInfo-basicInfoDftGrn',\n",
    "                 \n",
    "                 'EC@BasicInfo-BasicDftGrn',\n",
    "                 'EC@DT_min-DTDftGrn',\n",
    "                 \n",
    "                 'A1C@DT-DTDftGrn',\n",
    "                 'A1C@V-A1CNumeDftGrn',\n",
    "                 \n",
    "                 'Diag@DT-DTDftGrn',\n",
    "                 'Diag@Value-DiagDftGrn',\n",
    "                 \n",
    "                 'PNSectSent@Sentence-Tk@TknzLLMGrn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2ce4a0-6b52-4bb9-82f0-ac2359ee27ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ProcData/TensorFolder/Task2YearXXX\\P@age-AgeNumeDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\P@basicInfo-basicInfoDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\EC@BasicInfo-BasicDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\EC@DT_min-DTDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\A1C@DT-DTDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\A1C@V-A1CNumeDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\Diag@DT-DTDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\Diag@Value-DiagDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\PNSectSent@Sentence-Tk@TknzLLMGrn\n",
      "B-P@age-AgeNumeDftGrn_wgt (4, 19)\n",
      "B-P@basicInfo-basicInfoDftGrn_idx (4, 2)\n",
      "B-P-EC@BasicInfo-BasicDftGrn_idx (4, 25, 2)\n",
      "B-P-EC@DT_min-DTDftGrn_idx (4, 25, 7)\n",
      "B-P-EC-A1C@DT-DTDftGrn_idx (4, 25, 1, 7)\n",
      "B-P-EC-A1C@V-A1CNumeDftGrn_wgt (4, 25, 1, 37)\n",
      "B-P-EC-Diag@DT-DTDftGrn_idx (4, 25, 22, 7)\n",
      "B-P-EC-Diag@Value-DiagDftGrn_idx (4, 25, 22, 3)\n",
      "B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLMGrn_idx (4, 25, 1, 14, 121, 11)\n"
     ]
    }
   ],
   "source": [
    "batch_rfg = {}\n",
    "\n",
    "for recfldgrn in recfldgrn_list:\n",
    "    \n",
    "    # (1) get tensor_folder\n",
    "    tensor_folder = os.path.join(TaskTensor_folder, recfldgrn)\n",
    "    print(tensor_folder)\n",
    "\n",
    "    # (2) get df_Pat and full_recfldgrn\n",
    "    df_Pat = load_df_data_from_folder(tensor_folder).set_index('PID')\n",
    "    full_recfldgrn = df_Pat.columns[0]\n",
    "    suffix = full_recfldgrn.split('_')[-1]\n",
    "    assert recfldgrn in full_recfldgrn\n",
    "\n",
    "    # (3) load batch: TODO: convert this to DataSet and DataLoader\n",
    "    df_batch = df_Pat.loc[batch_PID_order]\n",
    "\n",
    "    # (4) tensor batch as tensor_idx\n",
    "    new_full_recfldgrn = 'B-' + full_recfldgrn\n",
    "    values_list = df_batch[full_recfldgrn].to_list()\n",
    "    suffix = full_recfldgrn.split('_')[-1]\n",
    "    # print(suffix)\n",
    "    # print(new_full_recfldgrn)\n",
    "    D = convert_relational_list_to_numpy(values_list, new_full_recfldgrn, suffix)\n",
    "    tensor_idx = D[new_full_recfldgrn]\n",
    "    \n",
    "    batch_rfg[new_full_recfldgrn] = tensor_idx\n",
    "    \n",
    "for k, v in batch_rfg.items(): print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ba8cdc-f621-4760-b1b4-61a326095105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-P@age-AgeNumeDftGrn_wgt',\n",
       " 'B-P@basicInfo-basicInfoDftGrn_idx',\n",
       " 'B-P-EC@BasicInfo-BasicDftGrn_idx',\n",
       " 'B-P-EC@DT_min-DTDftGrn_idx',\n",
       " 'B-P-EC-A1C@DT-DTDftGrn_idx',\n",
       " 'B-P-EC-A1C@V-A1CNumeDftGrn_wgt',\n",
       " 'B-P-EC-Diag@DT-DTDftGrn_idx',\n",
       " 'B-P-EC-Diag@Value-DiagDftGrn_idx',\n",
       " 'B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLMGrn_idx']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_recfldgrn_list = [k for k in batch_rfg]\n",
    "full_recfldgrn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193fa70c-78b3-4042-a278-799c05d92b03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "RECFLD_TO_TENSOR = {}\n",
    "\n",
    "for full_recfldgrn in full_recfldgrn_list:\n",
    "    # (1) get the info_raw from batch_rfg\n",
    "    info_raw = batch_rfg[full_recfldgrn]\n",
    "   \n",
    "    # (2) get the holder (input_idx) and holder_wgt (for nume embedding only)\n",
    "    if '_idx' in full_recfldgrn:\n",
    "        holder_wgt = 'Empty'\n",
    "        holder = torch.LongTensor(info_raw)\n",
    "    elif '_wgt' in full_recfldgrn:\n",
    "        holder_wgt = torch.FloatTensor(info_raw)\n",
    "        # ATTENTION: here holder_wgt could contain zeros in some valid positions.\n",
    "        holder = torch.ones_like(holder_wgt).cumsum(-1).masked_fill(holder_wgt == 0, 0).long()\n",
    "    else:\n",
    "        raise ValueError(f'Invalid suffix \"{suffix}\"')\n",
    "\n",
    "    info_dict = {'holder': holder, 'holder_wgt': holder_wgt}\n",
    "    \n",
    "    RECFLD_TO_TENSOR[full_recfldgrn] = info_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31abf433-cc9e-4001-8672-7c1594fd40f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubUnitName</th>\n",
       "      <th>input_names</th>\n",
       "      <th>output_name</th>\n",
       "      <th>output_layerid</th>\n",
       "      <th>SubUnit_BasicNN_List</th>\n",
       "      <th>SubUnit_DefaultBasicNN_List</th>\n",
       "      <th>SubUnit_BasicNN_Config_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P@age-AgeNumeDftGrn_wgt]</td>\n",
       "      <td>B-P@age-AgeNumeDft</td>\n",
       "      <td>3</td>\n",
       "      <td>[expander-NumeEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-NumeEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P@basicInfo-basicInfoDftGrn_idx]</td>\n",
       "      <td>B-P@basicInfo-basicInfoDft</td>\n",
       "      <td>3</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, 'Male': 1...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC@BasicInfo-BasicDftGrn_idx]</td>\n",
       "      <td>B-P-EC@BasicInfo-BasicDft</td>\n",
       "      <td>4</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, 'X': 1, '...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC@DT_min-DTDftGrn_idx]</td>\n",
       "      <td>B-P-EC@DT_min-DTDft</td>\n",
       "      <td>4</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, '_missing...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-A1C@DT-DTDftGrn_idx]</td>\n",
       "      <td>B-P-EC-A1C@DT-DTDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, '_missing...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-A1C@V-A1CNumeDftGrn_wgt]</td>\n",
       "      <td>B-P-EC-A1C@V-A1CNumeDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-NumeEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-NumeEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-Diag@DT-DTDftGrn_idx]</td>\n",
       "      <td>B-P-EC-Diag@DT-DTDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, '_missing...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-Diag@Value-DiagDftGrn_idx]</td>\n",
       "      <td>B-P-EC-Diag@Value-DiagDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, '_missing...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...</td>\n",
       "      <td>B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM</td>\n",
       "      <td>7</td>\n",
       "      <td>[expander-LLMEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': PreTrainedTokenizerFast(n...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-LLMEmbed', 'Bas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SubUnitName                                        input_names  \\\n",
       "0           E                        [B-P@age-AgeNumeDftGrn_wgt]   \n",
       "1           E                [B-P@basicInfo-basicInfoDftGrn_idx]   \n",
       "2           E                 [B-P-EC@BasicInfo-BasicDftGrn_idx]   \n",
       "3           E                       [B-P-EC@DT_min-DTDftGrn_idx]   \n",
       "4           E                       [B-P-EC-A1C@DT-DTDftGrn_idx]   \n",
       "5           E                   [B-P-EC-A1C@V-A1CNumeDftGrn_wgt]   \n",
       "6           E                      [B-P-EC-Diag@DT-DTDftGrn_idx]   \n",
       "7           E                 [B-P-EC-Diag@Value-DiagDftGrn_idx]   \n",
       "8           E  [B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...   \n",
       "\n",
       "                                       output_name  output_layerid  \\\n",
       "0                               B-P@age-AgeNumeDft               3   \n",
       "1                       B-P@basicInfo-basicInfoDft               3   \n",
       "2                        B-P-EC@BasicInfo-BasicDft               4   \n",
       "3                              B-P-EC@DT_min-DTDft               4   \n",
       "4                              B-P-EC-A1C@DT-DTDft               5   \n",
       "5                          B-P-EC-A1C@V-A1CNumeDft               5   \n",
       "6                             B-P-EC-Diag@DT-DTDft               5   \n",
       "7                        B-P-EC-Diag@Value-DiagDft               5   \n",
       "8  B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM               7   \n",
       "\n",
       "   SubUnit_BasicNN_List                        SubUnit_DefaultBasicNN_List  \\\n",
       "0  [expander-NumeEmbed]  [{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, 'Male': 1...   \n",
       "2  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, 'X': 1, '...   \n",
       "3  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, '_missing...   \n",
       "4  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, '_missing...   \n",
       "5  [expander-NumeEmbed]  [{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, '_missing...   \n",
       "7  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, '_missing...   \n",
       "8   [expander-LLMEmbed]  [{'vocab_tokenizer': PreTrainedTokenizerFast(n...   \n",
       "\n",
       "                         SubUnit_BasicNN_Config_List  \n",
       "0  [{'nn_type_nn_name': 'expander-NumeEmbed', 'Ba...  \n",
       "1  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "2  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "3  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "4  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "5  [{'nn_type_nn_name': 'expander-NumeEmbed', 'Ba...  \n",
       "6  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "7  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "8  [{'nn_type_nn_name': 'expander-LLMEmbed', 'Bas...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fieldnn.dataflowfn.embedflowfn import get_EmbeddingBlock_DataFlow\n",
    "\n",
    "from fieldnn.dataflowfn.baseflowfn import mapping_SubUnitName_to_SubUnitNNList\n",
    "from fieldnn.dataflowfn.baseflowfn import get_SubUnit_Default_NNPara_List\n",
    "from fieldnn.dataflowfn.baseflowfn import get_SubUnit_BasicNN_Config_List\n",
    "\n",
    "############################################# Hyperparameters\n",
    "default_BasicNNtype_To_NNName = {\n",
    "    'expander': None, # will be updated according to the Grn Type\n",
    "    'reducer': 'Max',\n",
    "    'merger': 'Merger',\n",
    "    'learner': None, # TODO: ignore this currently\n",
    "    \n",
    "}\n",
    "#############################################\n",
    "\n",
    "############################\n",
    "embed_size = 128\n",
    "process = {'activator': 'gelu',\n",
    "           'dropout': {'p': 0.5, 'inplace': False},\n",
    "           'layernorm': {'eps': 1e-05, 'elementwise_affine': True}}\n",
    "############################\n",
    "\n",
    "\n",
    "default_SubUnitName = 'E'\n",
    "fldgrn_folder = 'data/ProcData/FldGrnInfo'\n",
    "learner_default_dict = {} # To update it in the future. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_SubUnit = get_EmbeddingBlock_DataFlow(full_recfldgrn_list, default_SubUnitName)\n",
    "\n",
    "s = df_SubUnit.apply(lambda x: mapping_SubUnitName_to_SubUnitNNList(x['SubUnitName'], \n",
    "                                                                    x['input_names'],\n",
    "                                                                    default_BasicNNtype_To_NNName), \n",
    "                    axis = 1)\n",
    "df_SubUnit['SubUnit_BasicNN_List'] = s\n",
    "s = df_SubUnit.apply(lambda x: get_SubUnit_Default_NNPara_List(x['SubUnit_BasicNN_List'], \n",
    "                                                               x['input_names'],\n",
    "                                                               fldgrn_folder, \n",
    "                                                               learner_default_dict), axis = 1)\n",
    "\n",
    "df_SubUnit['SubUnit_DefaultBasicNN_List'] = s\n",
    "\n",
    "\n",
    "\n",
    "s = df_SubUnit.apply(lambda x: get_SubUnit_BasicNN_Config_List(x['SubUnit_BasicNN_List'], \n",
    "                                                               x['SubUnit_DefaultBasicNN_List'], \n",
    "                                                               x['input_names'], \n",
    "                                                               x['output_name'], \n",
    "                                                                embed_size, \n",
    "                                                                process, \n",
    "                                                               ), axis = 1)\n",
    "\n",
    "df_SubUnit['SubUnit_BasicNN_Config_List'] = s\n",
    "df_SubUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68881c4f-277c-4522-aae2-0f27d0f824eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-P@age-AgeNumeDft torch.Size([4, 19, 128])\n",
      "B-P@basicInfo-basicInfoDft torch.Size([4, 2, 128])\n",
      "B-P-EC@BasicInfo-BasicDft torch.Size([4, 25, 2, 128])\n",
      "B-P-EC@DT_min-DTDft torch.Size([4, 25, 7, 128])\n",
      "B-P-EC-A1C@DT-DTDft torch.Size([4, 25, 1, 7, 128])\n",
      "B-P-EC-A1C@V-A1CNumeDft torch.Size([4, 25, 1, 37, 128])\n",
      "B-P-EC-Diag@DT-DTDft torch.Size([4, 25, 22, 7, 128])\n",
      "B-P-EC-Diag@Value-DiagDft torch.Size([4, 25, 22, 3, 128])\n",
      "B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM torch.Size([4, 25, 1, 14, 121, 11, 128])\n"
     ]
    }
   ],
   "source": [
    "from fieldnn.module.embedblock import EmbedBlockLayer\n",
    "\n",
    "EmbedBlock = EmbedBlockLayer(df_SubUnit)\n",
    "\n",
    "RECFLD_TO_EMBEDTESNOR = EmbedBlock(RECFLD_TO_TENSOR)\n",
    "\n",
    "for k, v in RECFLD_TO_EMBEDTESNOR.items():\n",
    "    print(k, v['info'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751dd1d-0f73-4cd3-a461-47b845440601",
   "metadata": {},
   "source": [
    "# Get Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f19cc942-c754-4d64-a618-605709572c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-P@age-AgeNumeDft torch.Size([4, 19, 128])\n",
      "B-P@basicInfo-basicInfoDft torch.Size([4, 2, 128])\n",
      "B-P-EC@BasicInfo-BasicDft torch.Size([4, 25, 2, 128])\n",
      "B-P-EC@DT_min-DTDft torch.Size([4, 25, 7, 128])\n",
      "B-P-EC-A1C@DT-DTDft torch.Size([4, 25, 1, 7, 128])\n",
      "B-P-EC-A1C@V-A1CNumeDft torch.Size([4, 25, 1, 37, 128])\n",
      "B-P-EC-Diag@DT-DTDft torch.Size([4, 25, 22, 7, 128])\n",
      "B-P-EC-Diag@Value-DiagDft torch.Size([4, 25, 22, 3, 128])\n",
      "B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM torch.Size([4, 25, 1, 14, 121, 11, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for full_recfld, info_dict in RECFLD_TO_EMBEDTESNOR.items():\n",
    "    print(full_recfld, info_dict['info'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275aa97-0a80-4749-ae7c-6583364cbb73",
   "metadata": {},
   "source": [
    "# Reducer: BasicNN Name "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0485d-6f1f-466d-9f3c-1c72a859ad43",
   "metadata": {},
   "source": [
    "## ReducerSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc89a585-2020-4863-8c21-cb85709cedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ReduceSumLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReduceSumLayer, self).__init__()   \n",
    "\n",
    "    def forward(self, info, leng_mask):\n",
    "        # (bs, xxx, l, dim) --> (bs, xxx, dim)\n",
    "        info = torch.sum(info, -2)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d8228-81cf-4ecd-86ca-b304431ba839",
   "metadata": {},
   "source": [
    "## ReducerMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0774efbc-95ca-41e6-b627-2cadbc448e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ReduceMeanLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReduceMeanLayer, self).__init__()\n",
    "  \n",
    "    def forward(self, info, leng_mask):\n",
    "        leng = (leng_mask == 0).sum(-1).unsqueeze(-1).float()\n",
    "        leng[leng == 0.] = 1.0 # change pad to any non-zeros to be dominators.\n",
    "        info = torch.sum(info, -2) # (bs, xxx, l, dim) --> (bs, xxx, dim)\n",
    "        info = info/leng           # (bs, xxx, dim)    --> (bs, xxx, dim)\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e981190-4cb5-4d6a-8965-2107963572e5",
   "metadata": {},
   "source": [
    "## ReducerMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94aa259-296f-4d07-b2ed-32160eba9cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RecuderMaxLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecuderMaxLayer, self).__init__()\n",
    "\n",
    "    def forward(self, info, leng_mask):\n",
    "        info = info.masked_fill(leng_mask.unsqueeze(-1), -10000) # double check this.\n",
    "        a, b = info.max(-2) # not necessary, all the values could be smaller than 0.\n",
    "        return a\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb8bb9-2616-4efb-9b4e-4937bcc09422",
   "metadata": {},
   "source": [
    "## ReducerCat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c339a8e-357d-4b9e-b6f2-6db9b3a2452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "class ConcatenateLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConcatenateLayer, self).__init__()\n",
    "\n",
    "    def forward(self, info, leng_mask):\n",
    "        l, dim = info.shape[-2:]\n",
    "        info = info.view(*info.shape[:-2],  l*dim)\n",
    "        return info   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66597c-e677-4b67-89f6-4b84824cf9b3",
   "metadata": {},
   "source": [
    "# Reducer: Basic NN Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c552ac7-e2d2-4bab-9b4a-30b8b330efc5",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f51497f3-40f0-42b1-997f-e8efde268063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# from fieldnn.basicnn.reducer.rdcmean import ReduceMeanLayer\n",
    "# from fieldnn.basicnn.reducer.rdcsum import ReduceSumLayer\n",
    "# from fieldnn.basicnn.reducer.rdcmax import RecuderMaxLayer\n",
    "# from fieldnn.basicnn.reducer.rdccat import ConcatenateLayer\n",
    "\n",
    "# from .rdcmean import ReduceMeanLayer\n",
    "# from .rdcsum import ReduceSumLayer\n",
    "# from .rdcmax import RecuderMaxLayer\n",
    "# from .rdccat import ConcatenateLayer\n",
    "\n",
    "\n",
    "class Reducer_Layer(torch.nn.Module):\n",
    "    def __init__(self, input_names_nnlvl, output_name_nnlvl, reducer_layer_para):\n",
    "        super(Reducer_Layer, self).__init__()\n",
    "        \n",
    "        # Part 0: Meta\n",
    "        # here input_names and out_tensor just the tensor name, \n",
    "        # they are not the real tensors\n",
    "        # intead, the info_dict contains the corresponding real tensors.\n",
    "        assert len(input_names_nnlvl) == 1\n",
    "        self.input_names_nnlvl = input_names_nnlvl\n",
    "        self.input_name_nnlvl = input_names_nnlvl[0]\n",
    "        \n",
    "        # output_name should be generated from the input_names\n",
    "        self.output_name_nnlvl = output_name_nnlvl\n",
    "        # self.output_name = output_name\n",
    "\n",
    "        # the input feature dim size and output feature dim size\n",
    "        self.input_size = reducer_layer_para['input_size']\n",
    "        self.output_size = reducer_layer_para['output_size']\n",
    "\n",
    "        # Part 1: NN\n",
    "        nn_name, nn_para = reducer_layer_para['nn_name'], reducer_layer_para['nn_para']\n",
    "        if nn_name.lower() == 'mean':\n",
    "            self.reducer = ReduceMeanLayer()\n",
    "        elif nn_name.lower() == 'sum':\n",
    "            self.reducer = ReduceSumLayer()\n",
    "        elif nn_name.lower() == 'max':\n",
    "            self.reducer = RecuderMaxLayer()\n",
    "        elif nn_name.lower() == 'concat':\n",
    "            self.reducer = ConcatenateLayer()\n",
    "            # TODO: need to assert something\n",
    "            assert self.output_size % self.input_size == 0\n",
    "        else:\n",
    "            raise ValueError(f'There is no layer for \"{nn_name}\"')\n",
    "            \n",
    "        # Part 2: PostProcess\n",
    "        self.postprocess = torch.nn.ModuleDict()\n",
    "        for method, config in reducer_layer_para['postprocess'].items():\n",
    "            if method == 'activator':\n",
    "                activator = config\n",
    "                if activator.lower() == 'relu': \n",
    "                    self.postprocess[method] = torch.nn.ReLU()\n",
    "                elif activator.lower() == 'tanh': \n",
    "                    self.postprocess[method] = torch.nn.Tanh()\n",
    "                elif activator.lower() == 'gelu':\n",
    "                    self.postprocess[method] = torch.nn.GELU()\n",
    "            elif method == 'dropout':\n",
    "                self.postprocess[method] = torch.nn.Dropout(**config)\n",
    "            elif method == 'layernorm':\n",
    "                self.postprocess[method] = torch.nn.LayerNorm(self.output_size, **config)\n",
    "            \n",
    "    def forward(self, input_names_nnlvl, INPUTS_TO_INFODICT):\n",
    "        # information preparation.\n",
    "        # 'INPUTS_TO_INFODICT` will come from SubUnit Layer.\n",
    "        assert len(input_names_nnlvl) == 1\n",
    "        input_name_nnlvl = input_names_nnlvl[0]\n",
    "        assert self.input_name_nnlvl == input_name_nnlvl\n",
    "        \n",
    "        info_dict = INPUTS_TO_INFODICT[input_name_nnlvl]\n",
    "        holder, info = info_dict['holder'], info_dict['info']\n",
    "        \n",
    "        # print(holder.shape, info.shape)\n",
    "        # the following part is the data proprocessing\n",
    "        leng_mask = holder == 0\n",
    "        info = self.reducer(info, leng_mask)\n",
    "        \n",
    "        for name, layer in self.postprocess.items():\n",
    "            info = layer(info)\n",
    "            \n",
    "        holder = (leng_mask == 0).sum(-1)\n",
    "        \n",
    "        # output_name_nnlvl is not necessarily to be stored in the \n",
    "        return self.output_name_nnlvl, {'holder': holder, 'info': info}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb44ba-bf0b-4e5c-82e2-873608e3abaf",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1e65469-fdeb-426d-bab1-8c998e80ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reducer_para(nn_name, nn_para, input_size, output_size, postprocess):\n",
    "    reducer_layer_para = {}\n",
    "    reducer_layer_para['nn_type'] = 'reducer'\n",
    "    reducer_layer_para['nn_name'] = nn_name\n",
    "    reducer_layer_para['nn_para'] = nn_para\n",
    "    reducer_layer_para['input_size'] = input_size\n",
    "    reducer_layer_para['output_size'] = output_size\n",
    "    reducer_layer_para['postprocess'] = postprocess\n",
    "    return reducer_layer_para"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f036975-6ea8-475c-9983-9ecb492a7cc9",
   "metadata": {},
   "source": [
    "# Merger: BasicNN Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05706e15-03b6-455b-89d4-4a8ee186edd6",
   "metadata": {},
   "source": [
    "## Merger-Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26e99e3b-6c48-4302-afe2-11dd30c1bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MergeLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MergeLayer, self).__init__()\n",
    "    \n",
    "    def forward(self, tensor_list, order = -2):\n",
    "        info = torch.cat([i.unsqueeze(order) for i in tensor_list], order)\n",
    "        return info   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12f7f0-85cc-4df3-b324-bf9a4f225d4e",
   "metadata": {},
   "source": [
    "# Merger: BasicNN Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e90687-84dd-40be-a661-51c908674681",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e71f1bae-9112-402b-b4ab-e7796b554d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from .merge import MergeLayer\n",
    "\n",
    "class Merger_Layer(torch.nn.Module):\n",
    "    def __init__(self, input_names_nnlvl, output_name_nnlvl, merger_layer_para):\n",
    "        super(Merger_Layer, self).__init__()\n",
    "        \n",
    "        # the input_names_nnlvl\n",
    "        self.input_names_nnlvl = input_names_nnlvl\n",
    "        # output_name should be generated from the input_names\n",
    "        self.output_name_nnlvl = output_name_nnlvl\n",
    "        \n",
    "        self.input_size = merger_layer_para['input_size']\n",
    "        self.output_size = merger_layer_para['output_size']\n",
    "        \n",
    "        # Part 1: NN\n",
    "        nn_name = merger_layer_para['nn_name']\n",
    "        nn_para = merger_layer_para['nn_para']\n",
    "        \n",
    "        if nn_name.lower() == 'merger':\n",
    "            self.merger = MergeLayer()\n",
    "        else:\n",
    "            raise ValueError(f'The NN \"{nn_name}\" is not available')\n",
    "        \n",
    "        # Part 2: PostProcess\n",
    "        self.postprocess = torch.nn.ModuleDict()\n",
    "        for method, config in merger_layer_para['postprocess'].items():\n",
    "            if method == 'activator':\n",
    "                activator = config\n",
    "                if activator.lower() == 'relu': \n",
    "                    self.postprocess[method] = torch.nn.ReLU()\n",
    "                elif activator.lower() == 'tanh': \n",
    "                    self.postprocess[method] = torch.nn.Tanh()\n",
    "                elif activator.lower() == 'gelu':\n",
    "                    self.postprocess[method] = torch.nn.GELU()\n",
    "            elif method == 'dropout':\n",
    "                self.postprocess[method] = torch.nn.Dropout(**config)\n",
    "            elif method == 'layernorm':\n",
    "                self.postprocess[method] = torch.nn.LayerNorm(self.output_size, **config)\n",
    "                \n",
    "    def forward(self, input_names_nnlvl, INPUTS_TO_INFODICT):\n",
    "        \n",
    "        INPUTS = {k:v for k, v in INPUTS_TO_INFODICT.items() if k in input_names_nnlvl}\n",
    "\n",
    "        # (1) holder\n",
    "        holder = self.merger([data['holder'] for fld, data in INPUTS.items()], -1)\n",
    "        \n",
    "        # (2) merge data\n",
    "        info = self.merger([data['info'] for fld, data in INPUTS.items()], -2)\n",
    "        \n",
    "        # (3) post-process\n",
    "        for name, layer in self.postprocess.items():\n",
    "            info = layer(info)\n",
    "\n",
    "        return self.output_name_nnlvl, {'holder': holder, 'info': info}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08656374-24b8-4b11-9a90-fdc4851008a0",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f599815d-06c4-472b-ba36-bb072637e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merger_para(nn_name, nn_para, input_size, output_size, postprocess):\n",
    "    para = {}\n",
    "    para['nn_type'] = 'merger'\n",
    "    para['nn_name'] = nn_name\n",
    "    para['nn_para'] = nn_para\n",
    "    para['input_size'] = input_size\n",
    "    para['output_size'] = output_size\n",
    "    para['postprocess'] = postprocess\n",
    "    return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75575645-35fe-4303-93d6-abae7e7bacce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
