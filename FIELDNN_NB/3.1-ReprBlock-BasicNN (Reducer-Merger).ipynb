{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85de7ae-af36-4d22-8e78-6d53cf43ec6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/floydluo/Library/CloudStorage/GoogleDrive-jjluo@terpmail.umd.edu/My Drive/0-Research-Project/MedStar/MS_CODE/FieldNN\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6856af-fdcb-4d89-b714-e3774aed09c8",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "This is the for \n",
    "\n",
    "* module `fieldnn.basicnn.reducer` module\n",
    "* module `fieldnn.configfn.reducerfn` module\n",
    "* module `fieldnn.basicnn.merger` module\n",
    "* module `fieldnn.configfn.mergerfn` module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb79ef-c5ce-423d-bc4b-73b4cab4e43c",
   "metadata": {},
   "source": [
    "# Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66eb4992-cf60-4626-92df-e23c1f7890f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 <---- dataset\n",
      "1 <---- dataset\n",
      "B-P@PatEcInfo-InfoGrn_wgt torch.Size([4, 43])\n",
      "B-P@PatEcInfo-InfoGrn_tknidx torch.Size([4, 43])\n",
      "B-P@PatEcInfo-InfoGrn_fldidx torch.Size([4, 43])\n",
      "B-P-EC-PNSect@AllText-TknzGrn_wgt torch.Size([4, 23, 14, 221])\n",
      "B-P-EC-PNSect@AllText-TknzGrn_tknidx torch.Size([4, 23, 14, 221])\n",
      "B-P-EC-PNSect@AllText-TknzGrn_fldidx torch.Size([4, 23, 14, 221])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from fieldnn.dataset import RFGDataset, my_collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "Tensor_folder = 'data/ProcData/FldGrnTensor/'\n",
    "recfldgrn_list = ['P@PatEcInfo-InfoGrn',  'P-EC-PNSect@AllText-TknzGrn']\n",
    "full_recfldgrn_list = ['B-' + i for i in recfldgrn_list]\n",
    "\n",
    "# from the get_grain_fn to get the Elig_Set.\n",
    "Elig_Set = ['P4', 'P5', 'P6', 'P7']\n",
    "\n",
    "dataset = RFGDataset(Tensor_folder, recfldgrn_list, Elig_Set, RecRootID = 'PID')\n",
    "print(len(dataset), '<---- dataset')\n",
    "dataloader = DataLoader(dataset, batch_size = 4, shuffle = True, collate_fn = my_collate_fn)\n",
    "print(len(dataloader), '<---- dataset')\n",
    "\n",
    "\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    # print(f'\\n------ {idx}')\n",
    "    batch_rfg, batch_y = batch\n",
    "    for k, v in batch_rfg.items(): print(k, v.shape)\n",
    "    break\n",
    "    # for k, v in batch_rfg.items(): print(k, v.shape)\n",
    "    # print(batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2ce4a0-6b52-4bb9-82f0-ac2359ee27ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-P@PatEcInfo-InfoGrn', 'B-P-EC-PNSect@AllText-TknzGrn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the Input\n",
    "RECFLD_TO_TENSOR = {}\n",
    "for full_recfldgrn in full_recfldgrn_list:\n",
    "    RECFLD_TO_TENSOR[full_recfldgrn] = {k: v for k, v in batch_rfg.items() if full_recfldgrn in k}\n",
    "    \n",
    "[i for i in RECFLD_TO_TENSOR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31abf433-cc9e-4001-8672-7c1594fd40f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubUnitName</th>\n",
       "      <th>input_names</th>\n",
       "      <th>output_name</th>\n",
       "      <th>output_layerid</th>\n",
       "      <th>SubUnit_BasicNN_List</th>\n",
       "      <th>SubUnit_DefaultBasicNN_List</th>\n",
       "      <th>SubUnit_BasicNN_Config_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P@PatEcInfo-InfoGrn]</td>\n",
       "      <td>B-P@PatEcInfo-Info</td>\n",
       "      <td>3</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'full_recfldgrn': 'B-P@PatEcInfo-InfoGrn', '...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-PNSect@AllText-TknzGrn]</td>\n",
       "      <td>B-P-EC-PNSect@AllText-Tknz</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-LLMEmbed]</td>\n",
       "      <td>[{'full_recfldgrn': 'B-P-EC-PNSect@AllText-Tkn...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-LLMEmbed', 'Bas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SubUnitName                      input_names                 output_name  \\\n",
       "0           E          [B-P@PatEcInfo-InfoGrn]          B-P@PatEcInfo-Info   \n",
       "1           E  [B-P-EC-PNSect@AllText-TknzGrn]  B-P-EC-PNSect@AllText-Tknz   \n",
       "\n",
       "   output_layerid  SubUnit_BasicNN_List  \\\n",
       "0               3  [expander-CateEmbed]   \n",
       "1               5   [expander-LLMEmbed]   \n",
       "\n",
       "                         SubUnit_DefaultBasicNN_List  \\\n",
       "0  [{'full_recfldgrn': 'B-P@PatEcInfo-InfoGrn', '...   \n",
       "1  [{'full_recfldgrn': 'B-P-EC-PNSect@AllText-Tkn...   \n",
       "\n",
       "                         SubUnit_BasicNN_Config_List  \n",
       "0  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "1  [{'nn_type_nn_name': 'expander-LLMEmbed', 'Bas...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fieldnn.dataflowfn.embedflowfn import get_EmbeddingBlock_SubUnit\n",
    "\n",
    "from fieldnn.dataflowfn.baseflowfn import mapping_SubUnitName_to_SubUnitNNList\n",
    "from fieldnn.dataflowfn.baseflowfn import get_SubUnit_Default_NNPara_List\n",
    "from fieldnn.dataflowfn.baseflowfn import get_SubUnit_BasicNN_Config_List\n",
    "\n",
    "############################################# Hyperparameters\n",
    "default_BasicNNtype_To_NNName = {\n",
    "    'expander': None, # will be updated according to the Grn Type\n",
    "    'reducer': 'Max',\n",
    "    'merger': 'Merger',\n",
    "    'learner': None, # TODO: ignore this currently\n",
    "    \n",
    "}\n",
    "#############################################\n",
    "\n",
    "############################\n",
    "embed_size = 128\n",
    "process = {'activator': 'gelu',\n",
    "           'dropout': {'p': 0.5, 'inplace': False},\n",
    "           'layernorm': {'eps': 1e-05, 'elementwise_affine': True}}\n",
    "############################\n",
    "\n",
    "\n",
    "default_SubUnitName = 'E'\n",
    "fldgrn_folder = 'data/ProcData/FldGrnInfo'\n",
    "learner_default_dict = {} # To update it in the future. \n",
    "\n",
    "\n",
    "df_SubUnit = get_EmbeddingBlock_SubUnit(full_recfldgrn_list, default_SubUnitName)\n",
    "\n",
    "s = df_SubUnit.apply(lambda x: mapping_SubUnitName_to_SubUnitNNList(x['SubUnitName'], \n",
    "                                                                    x['input_names'],\n",
    "                                                                    default_BasicNNtype_To_NNName), \n",
    "                    axis = 1)\n",
    "df_SubUnit['SubUnit_BasicNN_List'] = s\n",
    "s = df_SubUnit.apply(lambda x: get_SubUnit_Default_NNPara_List(x['SubUnit_BasicNN_List'], \n",
    "                                                               x['input_names'],\n",
    "                                                               fldgrn_folder, \n",
    "                                                               learner_default_dict), axis = 1)\n",
    "\n",
    "df_SubUnit['SubUnit_DefaultBasicNN_List'] = s\n",
    "\n",
    "\n",
    "\n",
    "s = df_SubUnit.apply(lambda x: get_SubUnit_BasicNN_Config_List(x['SubUnit_BasicNN_List'], \n",
    "                                                               x['SubUnit_DefaultBasicNN_List'], \n",
    "                                                               x['input_names'], \n",
    "                                                               x['output_name'], \n",
    "                                                                embed_size, \n",
    "                                                                process, \n",
    "                                                               ), axis = 1)\n",
    "\n",
    "df_SubUnit['SubUnit_BasicNN_Config_List'] = s\n",
    "df_SubUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68881c4f-277c-4522-aae2-0f27d0f824eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-P@PatEcInfo-Info torch.Size([4, 43, 128])\n",
      "B-P-EC-PNSect@AllText-Tknz torch.Size([4, 23, 14, 221, 128])\n"
     ]
    }
   ],
   "source": [
    "from fieldnn.module.embedblock import EmbedBlockLayer\n",
    "\n",
    "EmbedBlock = EmbedBlockLayer(df_SubUnit)\n",
    "\n",
    "RECFLD_TO_EMBEDTESNOR = EmbedBlock(RECFLD_TO_TENSOR)\n",
    "\n",
    "for k, v in RECFLD_TO_EMBEDTESNOR.items():\n",
    "    print(k, v['info'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751dd1d-0f73-4cd3-a461-47b845440601",
   "metadata": {},
   "source": [
    "# Get Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f19cc942-c754-4d64-a618-605709572c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-P@PatEcInfo-Info torch.Size([4, 43, 128])\n",
      "B-P-EC-PNSect@AllText-Tknz torch.Size([4, 23, 14, 221, 128])\n"
     ]
    }
   ],
   "source": [
    "for full_recfld, info_dict in RECFLD_TO_EMBEDTESNOR.items():\n",
    "    print(full_recfld, info_dict['info'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275aa97-0a80-4749-ae7c-6583364cbb73",
   "metadata": {},
   "source": [
    "# Reducer: BasicNN Name "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0485d-6f1f-466d-9f3c-1c72a859ad43",
   "metadata": {},
   "source": [
    "## ReducerSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc89a585-2020-4863-8c21-cb85709cedfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ReduceSumLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReduceSumLayer, self).__init__()   \n",
    "\n",
    "    def forward(self, info, leng_mask):\n",
    "        # (bs, xxx, l, dim) --> (bs, xxx, dim)\n",
    "        info = torch.sum(info, -2)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d8228-81cf-4ecd-86ca-b304431ba839",
   "metadata": {},
   "source": [
    "## ReducerMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0774efbc-95ca-41e6-b627-2cadbc448e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ReduceMeanLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReduceMeanLayer, self).__init__()\n",
    "  \n",
    "    def forward(self, info, leng_mask):\n",
    "        leng = (leng_mask == 0).sum(-1).unsqueeze(-1).float()\n",
    "        leng[leng == 0.] = 1.0 # change pad to any non-zeros to be dominators.\n",
    "        info = torch.sum(info, -2) # (bs, xxx, l, dim) --> (bs, xxx, dim)\n",
    "        info = info/leng           # (bs, xxx, dim)    --> (bs, xxx, dim)\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e981190-4cb5-4d6a-8965-2107963572e5",
   "metadata": {},
   "source": [
    "## ReducerMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f94aa259-296f-4d07-b2ed-32160eba9cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RecuderMaxLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecuderMaxLayer, self).__init__()\n",
    "\n",
    "    def forward(self, info, leng_mask):\n",
    "        info = info.masked_fill(leng_mask.unsqueeze(-1), -10000) # double check this.\n",
    "        a, b = info.max(-2) # not necessary, all the values could be smaller than 0.\n",
    "        return a\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb8bb9-2616-4efb-9b4e-4937bcc09422",
   "metadata": {},
   "source": [
    "## ReducerCat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c339a8e-357d-4b9e-b6f2-6db9b3a2452d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "class ConcatenateLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConcatenateLayer, self).__init__()\n",
    "\n",
    "    def forward(self, info, leng_mask):\n",
    "        l, dim = info.shape[-2:]\n",
    "        info = info.view(*info.shape[:-2],  l*dim)\n",
    "        return info   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66597c-e677-4b67-89f6-4b84824cf9b3",
   "metadata": {},
   "source": [
    "# Reducer: Basic NN Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c552ac7-e2d2-4bab-9b4a-30b8b330efc5",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f51497f3-40f0-42b1-997f-e8efde268063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# from fieldnn.basicnn.reducer.rdcmean import ReduceMeanLayer\n",
    "# from fieldnn.basicnn.reducer.rdcsum import ReduceSumLayer\n",
    "# from fieldnn.basicnn.reducer.rdcmax import RecuderMaxLayer\n",
    "# from fieldnn.basicnn.reducer.rdccat import ConcatenateLayer\n",
    "\n",
    "# from .rdcmean import ReduceMeanLayer\n",
    "# from .rdcsum import ReduceSumLayer\n",
    "# from .rdcmax import RecuderMaxLayer\n",
    "# from .rdccat import ConcatenateLayer\n",
    "\n",
    "\n",
    "class Reducer_Layer(torch.nn.Module):\n",
    "    def __init__(self, input_names_nnlvl, output_name_nnlvl, reducer_layer_para):\n",
    "        super(Reducer_Layer, self).__init__()\n",
    "        \n",
    "        # Part 0: Meta\n",
    "        # here input_names and out_tensor just the tensor name, \n",
    "        # intead, the info_dict contains the corresponding real tensors.\n",
    "        assert len(input_names_nnlvl) == 1\n",
    "        self.input_names_nnlvl = input_names_nnlvl\n",
    "        self.input_name_nnlvl = input_names_nnlvl[0]\n",
    "        \n",
    "        # output_name should be generated from the input_names\n",
    "        self.output_name_nnlvl = output_name_nnlvl\n",
    "        \n",
    "        # the input feature dim size and output feature dim size\n",
    "        self.input_size = reducer_layer_para['input_size']\n",
    "        self.output_size = reducer_layer_para['output_size']\n",
    "\n",
    "        # Part 1: NN\n",
    "        nn_name, nn_para = reducer_layer_para['nn_name'], reducer_layer_para['nn_para']\n",
    "        if nn_name.lower() == 'mean':\n",
    "            self.reducer = ReduceMeanLayer()\n",
    "        elif nn_name.lower() == 'sum':\n",
    "            self.reducer = ReduceSumLayer()\n",
    "        elif nn_name.lower() == 'max':\n",
    "            self.reducer = RecuderMaxLayer()\n",
    "        elif nn_name.lower() == 'concat':\n",
    "            self.reducer = ConcatenateLayer()\n",
    "            # TODO: need to assert something\n",
    "            assert self.output_size % self.input_size == 0\n",
    "        else:\n",
    "            raise ValueError(f'There is no layer for \"{nn_name}\"')\n",
    "            \n",
    "        # Part 2: PostProcess\n",
    "        self.postprocess = torch.nn.ModuleDict()\n",
    "        for method, config in reducer_layer_para['postprocess'].items():\n",
    "            if method == 'activator':\n",
    "                activator = config\n",
    "                if activator.lower() == 'relu': \n",
    "                    self.postprocess[method] = torch.nn.ReLU()\n",
    "                elif activator.lower() == 'tanh': \n",
    "                    self.postprocess[method] = torch.nn.Tanh()\n",
    "                elif activator.lower() == 'gelu':\n",
    "                    self.postprocess[method] = torch.nn.GELU()\n",
    "            elif method == 'dropout':\n",
    "                self.postprocess[method] = torch.nn.Dropout(**config)\n",
    "            elif method == 'layernorm':\n",
    "                self.postprocess[method] = torch.nn.LayerNorm(self.output_size, **config)\n",
    "            \n",
    "    def forward(self, input_names_nnlvl, INPUTS_TO_INFODICT):\n",
    "        # information preparation.\n",
    "        # 'INPUTS_TO_INFODICT` will come from SubUnit Layer.\n",
    "        assert len(input_names_nnlvl) == 1\n",
    "        input_name_nnlvl = input_names_nnlvl[0]\n",
    "        assert self.input_name_nnlvl == input_name_nnlvl\n",
    "        \n",
    "        info_dict = INPUTS_TO_INFODICT[input_name_nnlvl]\n",
    "        holder, info = info_dict['holder'], info_dict['info']\n",
    "        \n",
    "        # print(holder.shape, info.shape)\n",
    "        # the following part is the data proprocessing\n",
    "        leng_mask = holder == 0\n",
    "        info = self.reducer(info, leng_mask)\n",
    "        \n",
    "        for name, layer in self.postprocess.items():\n",
    "            info = layer(info)\n",
    "            \n",
    "        holder = (leng_mask == 0).sum(-1)\n",
    "        \n",
    "        # output_name_nnlvl is not necessarily to be stored in the \n",
    "        return self.output_name_nnlvl, {'holder': holder, 'info': info}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb44ba-bf0b-4e5c-82e2-873608e3abaf",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1e65469-fdeb-426d-bab1-8c998e80ad81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_reducer_para(nn_name, nn_para, input_size, output_size, postprocess):\n",
    "    reducer_layer_para = {}\n",
    "    reducer_layer_para['nn_type'] = 'reducer'\n",
    "    reducer_layer_para['nn_name'] = nn_name\n",
    "    reducer_layer_para['nn_para'] = nn_para\n",
    "    reducer_layer_para['input_size'] = input_size\n",
    "    reducer_layer_para['output_size'] = output_size\n",
    "    reducer_layer_para['postprocess'] = postprocess\n",
    "    return reducer_layer_para"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f036975-6ea8-475c-9983-9ecb492a7cc9",
   "metadata": {},
   "source": [
    "# Merger: BasicNN Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05706e15-03b6-455b-89d4-4a8ee186edd6",
   "metadata": {},
   "source": [
    "## Merger-Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26e99e3b-6c48-4302-afe2-11dd30c1bdc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MergeLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MergeLayer, self).__init__()\n",
    "    \n",
    "    def forward(self, tensor_list, order = -2):\n",
    "        info = torch.cat([i.unsqueeze(order) for i in tensor_list], order)\n",
    "        return info   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12f7f0-85cc-4df3-b324-bf9a4f225d4e",
   "metadata": {},
   "source": [
    "# Merger: BasicNN Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e90687-84dd-40be-a661-51c908674681",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e71f1bae-9112-402b-b4ab-e7796b554d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# from .merge import MergeLayer\n",
    "\n",
    "class Merger_Layer(torch.nn.Module):\n",
    "    def __init__(self, input_names_nnlvl, output_name_nnlvl, merger_layer_para):\n",
    "        super(Merger_Layer, self).__init__()\n",
    "        \n",
    "        # the input_names_nnlvl\n",
    "        self.input_names_nnlvl = input_names_nnlvl\n",
    "        # output_name should be generated from the input_names\n",
    "        self.output_name_nnlvl = output_name_nnlvl\n",
    "        \n",
    "        self.input_size = merger_layer_para['input_size']\n",
    "        self.output_size = merger_layer_para['output_size']\n",
    "        \n",
    "        # Part 1: NN\n",
    "        nn_name = merger_layer_para['nn_name']\n",
    "        nn_para = merger_layer_para['nn_para']\n",
    "        \n",
    "        if nn_name.lower() == 'merger':\n",
    "            self.merger = MergeLayer()\n",
    "        else:\n",
    "            raise ValueError(f'The NN \"{nn_name}\" is not available')\n",
    "        \n",
    "        # Part 2: PostProcess\n",
    "        self.postprocess = torch.nn.ModuleDict()\n",
    "        for method, config in merger_layer_para['postprocess'].items():\n",
    "            if method == 'activator':\n",
    "                activator = config\n",
    "                if activator.lower() == 'relu': \n",
    "                    self.postprocess[method] = torch.nn.ReLU()\n",
    "                elif activator.lower() == 'tanh': \n",
    "                    self.postprocess[method] = torch.nn.Tanh()\n",
    "                elif activator.lower() == 'gelu':\n",
    "                    self.postprocess[method] = torch.nn.GELU()\n",
    "            elif method == 'dropout':\n",
    "                self.postprocess[method] = torch.nn.Dropout(**config)\n",
    "            elif method == 'layernorm':\n",
    "                self.postprocess[method] = torch.nn.LayerNorm(self.output_size, **config)\n",
    "                \n",
    "    def forward(self, input_names_nnlvl, INPUTS_TO_INFODICT):\n",
    "        \n",
    "        INPUTS = {k:v for k, v in INPUTS_TO_INFODICT.items() if k in input_names_nnlvl}\n",
    "\n",
    "        # (1) holder\n",
    "        holder = self.merger([data['holder'] for fld, data in INPUTS.items()], -1)\n",
    "        \n",
    "        # (2) merge data\n",
    "        info = self.merger([data['info'] for fld, data in INPUTS.items()], -2)\n",
    "        \n",
    "        # (3) post-process\n",
    "        for name, layer in self.postprocess.items():\n",
    "            info = layer(info)\n",
    "\n",
    "        return self.output_name_nnlvl, {'holder': holder, 'info': info}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08656374-24b8-4b11-9a90-fdc4851008a0",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f599815d-06c4-472b-ba36-bb072637e41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_merger_para(nn_name, nn_para, input_size, output_size, postprocess):\n",
    "    para = {}\n",
    "    para['nn_type'] = 'merger'\n",
    "    para['nn_name'] = nn_name\n",
    "    para['nn_para'] = nn_para\n",
    "    para['input_size'] = input_size\n",
    "    para['output_size'] = output_size\n",
    "    para['postprocess'] = postprocess\n",
    "    return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75575645-35fe-4303-93d6-abae7e7bacce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293fa67d-972d-438e-9c3f-25a61fffe0da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30c626-2c7f-4306-90c0-82117541bc88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
