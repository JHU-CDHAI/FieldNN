{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7588aa63-50a8-4f68-b40d-5e65e1783790",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\My Drive\\0-Research-Project\\MedStar\\MS_CODE\\FieldNN\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61bc989-f648-42f9-9580-fd91c5fddc0a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "This is the for \n",
    "\n",
    "* module `fieldnn.basicnn.learner` module\n",
    "* module `fieldnn.configfn.learnerfn` module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f1850-8d92-40fd-ba69-6657f63ca703",
   "metadata": {},
   "source": [
    "# Prepare Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e6de85-919d-4ffa-b1d6-0f286e9dc6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ProcData/TensorFolder/Task2YearXXX\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from recfldgrn.datapoint import load_df_data_from_folder\n",
    "from fieldnn.utils.layerfn import traverse, convert_relational_list_to_numpy\n",
    "\n",
    "###################### take this as given\n",
    "batch_PID_order = ['P1', 'P4', 'P5', 'P6']\n",
    "######################\n",
    "\n",
    "TaskTensor_folder = 'data/ProcData/TensorFolder/Task2YearXXX'\n",
    "print(TaskTensor_folder)\n",
    "\n",
    "l = sorted([i for i in os.listdir(TaskTensor_folder) if 'Grn' in i])\n",
    "# l\n",
    "\n",
    "recfldgrn_list =['P@age-AgeNumeDftGrn',\n",
    "                 'P@basicInfo-basicInfoDftGrn',\n",
    "                 \n",
    "                 'EC@BasicInfo-BasicDftGrn',\n",
    "                 'EC@DT_min-DTDftGrn',\n",
    "                 \n",
    "                 'A1C@DT-DTDftGrn',\n",
    "                 'A1C@V-A1CNumeDftGrn',\n",
    "                 \n",
    "                 'Diag@DT-DTDftGrn',\n",
    "                 'Diag@Value-DiagDftGrn',\n",
    "                 \n",
    "                 'PNSectSent@Sentence-Tk@TknzLLMGrn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70729ebd-a267-4acc-96e6-ef0ea85eb354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ProcData/TensorFolder/Task2YearXXX\\P@age-AgeNumeDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\P@basicInfo-basicInfoDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\EC@BasicInfo-BasicDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\EC@DT_min-DTDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\A1C@DT-DTDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\A1C@V-A1CNumeDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\Diag@DT-DTDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\Diag@Value-DiagDftGrn\n",
      "data/ProcData/TensorFolder/Task2YearXXX\\PNSectSent@Sentence-Tk@TknzLLMGrn\n",
      "B-P@age-AgeNumeDftGrn_wgt (4, 19)\n",
      "B-P@basicInfo-basicInfoDftGrn_idx (4, 2)\n",
      "B-P-EC@BasicInfo-BasicDftGrn_idx (4, 25, 2)\n",
      "B-P-EC@DT_min-DTDftGrn_idx (4, 25, 7)\n",
      "B-P-EC-A1C@DT-DTDftGrn_idx (4, 25, 1, 7)\n",
      "B-P-EC-A1C@V-A1CNumeDftGrn_wgt (4, 25, 1, 37)\n",
      "B-P-EC-Diag@DT-DTDftGrn_idx (4, 25, 22, 7)\n",
      "B-P-EC-Diag@Value-DiagDftGrn_idx (4, 25, 22, 3)\n",
      "B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLMGrn_idx (4, 25, 1, 14, 121, 11)\n"
     ]
    }
   ],
   "source": [
    "batch_rfg = {}\n",
    "\n",
    "for recfldgrn in recfldgrn_list:\n",
    "    \n",
    "    # (1) get tensor_folder\n",
    "    tensor_folder = os.path.join(TaskTensor_folder, recfldgrn)\n",
    "    print(tensor_folder)\n",
    "\n",
    "    # (2) get df_Pat and full_recfldgrn\n",
    "    df_Pat = load_df_data_from_folder(tensor_folder).set_index('PID')\n",
    "    full_recfldgrn = df_Pat.columns[0]\n",
    "    suffix = full_recfldgrn.split('_')[-1]\n",
    "    assert recfldgrn in full_recfldgrn\n",
    "\n",
    "    # (3) load batch: TODO: convert this to DataSet and DataLoader\n",
    "    df_batch = df_Pat.loc[batch_PID_order]\n",
    "\n",
    "    # (4) tensor batch as tensor_idx\n",
    "    new_full_recfldgrn = 'B-' + full_recfldgrn\n",
    "    values_list = df_batch[full_recfldgrn].to_list()\n",
    "    suffix = full_recfldgrn.split('_')[-1]\n",
    "    # print(suffix)\n",
    "    # print(new_full_recfldgrn)\n",
    "    D = convert_relational_list_to_numpy(values_list, new_full_recfldgrn, suffix)\n",
    "    tensor_idx = D[new_full_recfldgrn]\n",
    "    \n",
    "    batch_rfg[new_full_recfldgrn] = tensor_idx\n",
    "    \n",
    "for k, v in batch_rfg.items(): print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d944f1c-10e3-4e0d-a71e-40cd99cdc87f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-P@age-AgeNumeDftGrn_wgt',\n",
       " 'B-P@basicInfo-basicInfoDftGrn_idx',\n",
       " 'B-P-EC@BasicInfo-BasicDftGrn_idx',\n",
       " 'B-P-EC@DT_min-DTDftGrn_idx',\n",
       " 'B-P-EC-A1C@DT-DTDftGrn_idx',\n",
       " 'B-P-EC-A1C@V-A1CNumeDftGrn_wgt',\n",
       " 'B-P-EC-Diag@DT-DTDftGrn_idx',\n",
       " 'B-P-EC-Diag@Value-DiagDftGrn_idx',\n",
       " 'B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLMGrn_idx']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_recfldgrn_list = [k for k in batch_rfg]\n",
    "full_recfldgrn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "148b0bee-5e48-47e4-abb7-eef0f9638c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "RECFLD_TO_TENSOR = {}\n",
    "\n",
    "for full_recfldgrn in full_recfldgrn_list:\n",
    "    # (1) get the info_raw from batch_rfg\n",
    "    info_raw = batch_rfg[full_recfldgrn]\n",
    "   \n",
    "    # (2) get the holder (input_idx) and holder_wgt (for nume embedding only)\n",
    "    if '_idx' in full_recfldgrn:\n",
    "        holder_wgt = 'Empty'\n",
    "        holder = torch.LongTensor(info_raw)\n",
    "    elif '_wgt' in full_recfldgrn:\n",
    "        holder_wgt = torch.FloatTensor(info_raw)\n",
    "        # ATTENTION: here holder_wgt could contain zeros in some valid positions.\n",
    "        holder = torch.ones_like(holder_wgt).cumsum(-1).masked_fill(holder_wgt == 0, 0).long()\n",
    "    else:\n",
    "        raise ValueError(f'Invalid suffix \"{suffix}\"')\n",
    "\n",
    "    info_dict = {'holder': holder, 'holder_wgt': holder_wgt}\n",
    "    \n",
    "    RECFLD_TO_TENSOR[full_recfldgrn] = info_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f98dd28e-0fb5-43b2-ae78-5b07f07b814a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubUnitName</th>\n",
       "      <th>input_names</th>\n",
       "      <th>output_name</th>\n",
       "      <th>output_layerid</th>\n",
       "      <th>SubUnit_BasicNN_List</th>\n",
       "      <th>SubUnit_DefaultBasicNN_List</th>\n",
       "      <th>SubUnit_BasicNN_Config_List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P@age-AgeNumeDftGrn_wgt]</td>\n",
       "      <td>B-P@age-AgeNumeDft</td>\n",
       "      <td>3</td>\n",
       "      <td>[expander-NumeEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-NumeEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P@basicInfo-basicInfoDftGrn_idx]</td>\n",
       "      <td>B-P@basicInfo-basicInfoDft</td>\n",
       "      <td>3</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, 'Male': 1...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC@BasicInfo-BasicDftGrn_idx]</td>\n",
       "      <td>B-P-EC@BasicInfo-BasicDft</td>\n",
       "      <td>4</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, 'X': 1, '...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC@DT_min-DTDftGrn_idx]</td>\n",
       "      <td>B-P-EC@DT_min-DTDft</td>\n",
       "      <td>4</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, '_missing...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-A1C@DT-DTDftGrn_idx]</td>\n",
       "      <td>B-P-EC-A1C@DT-DTDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, '_missing...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-A1C@V-A1CNumeDftGrn_wgt]</td>\n",
       "      <td>B-P-EC-A1C@V-A1CNumeDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-NumeEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-NumeEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-Diag@DT-DTDftGrn_idx]</td>\n",
       "      <td>B-P-EC-Diag@DT-DTDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, '_missing...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-Diag@Value-DiagDftGrn_idx]</td>\n",
       "      <td>B-P-EC-Diag@Value-DiagDft</td>\n",
       "      <td>5</td>\n",
       "      <td>[expander-CateEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': {'_padding': 0, '_missing...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E</td>\n",
       "      <td>[B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...</td>\n",
       "      <td>B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM</td>\n",
       "      <td>7</td>\n",
       "      <td>[expander-LLMEmbed]</td>\n",
       "      <td>[{'vocab_tokenizer': PreTrainedTokenizerFast(n...</td>\n",
       "      <td>[{'nn_type_nn_name': 'expander-LLMEmbed', 'Bas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SubUnitName                                        input_names  \\\n",
       "0           E                        [B-P@age-AgeNumeDftGrn_wgt]   \n",
       "1           E                [B-P@basicInfo-basicInfoDftGrn_idx]   \n",
       "2           E                 [B-P-EC@BasicInfo-BasicDftGrn_idx]   \n",
       "3           E                       [B-P-EC@DT_min-DTDftGrn_idx]   \n",
       "4           E                       [B-P-EC-A1C@DT-DTDftGrn_idx]   \n",
       "5           E                   [B-P-EC-A1C@V-A1CNumeDftGrn_wgt]   \n",
       "6           E                      [B-P-EC-Diag@DT-DTDftGrn_idx]   \n",
       "7           E                 [B-P-EC-Diag@Value-DiagDftGrn_idx]   \n",
       "8           E  [B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzL...   \n",
       "\n",
       "                                       output_name  output_layerid  \\\n",
       "0                               B-P@age-AgeNumeDft               3   \n",
       "1                       B-P@basicInfo-basicInfoDft               3   \n",
       "2                        B-P-EC@BasicInfo-BasicDft               4   \n",
       "3                              B-P-EC@DT_min-DTDft               4   \n",
       "4                              B-P-EC-A1C@DT-DTDft               5   \n",
       "5                          B-P-EC-A1C@V-A1CNumeDft               5   \n",
       "6                             B-P-EC-Diag@DT-DTDft               5   \n",
       "7                        B-P-EC-Diag@Value-DiagDft               5   \n",
       "8  B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM               7   \n",
       "\n",
       "   SubUnit_BasicNN_List                        SubUnit_DefaultBasicNN_List  \\\n",
       "0  [expander-NumeEmbed]  [{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, 'Male': 1...   \n",
       "2  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, 'X': 1, '...   \n",
       "3  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, '_missing...   \n",
       "4  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, '_missing...   \n",
       "5  [expander-NumeEmbed]  [{'vocab_tokenizer': [1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, '_missing...   \n",
       "7  [expander-CateEmbed]  [{'vocab_tokenizer': {'_padding': 0, '_missing...   \n",
       "8   [expander-LLMEmbed]  [{'vocab_tokenizer': PreTrainedTokenizerFast(n...   \n",
       "\n",
       "                         SubUnit_BasicNN_Config_List  \n",
       "0  [{'nn_type_nn_name': 'expander-NumeEmbed', 'Ba...  \n",
       "1  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "2  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "3  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "4  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "5  [{'nn_type_nn_name': 'expander-NumeEmbed', 'Ba...  \n",
       "6  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "7  [{'nn_type_nn_name': 'expander-CateEmbed', 'Ba...  \n",
       "8  [{'nn_type_nn_name': 'expander-LLMEmbed', 'Bas...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fieldnn.dataflowfn.embedflowfn import get_EmbeddingBlock_DataFlow\n",
    "\n",
    "from fieldnn.dataflowfn.baseflowfn import mapping_SubUnitName_to_SubUnitNNList\n",
    "from fieldnn.dataflowfn.baseflowfn import get_SubUnit_Default_NNPara_List\n",
    "from fieldnn.dataflowfn.baseflowfn import get_SubUnit_BasicNN_Config_List\n",
    "\n",
    "############################################# Hyperparameters\n",
    "default_BasicNNtype_To_NNName = {\n",
    "    'expander': None, # will be updated according to the Grn Type\n",
    "    'reducer': 'Max',\n",
    "    'merger': 'Merger',\n",
    "    'learner': None, # TODO: ignore this currently\n",
    "    \n",
    "}\n",
    "#############################################\n",
    "\n",
    "############################\n",
    "embed_size = 128\n",
    "process = {'activator': 'gelu',\n",
    "           'dropout': {'p': 0.5, 'inplace': False},\n",
    "           'layernorm': {'eps': 1e-05, 'elementwise_affine': True}}\n",
    "############################\n",
    "\n",
    "\n",
    "default_SubUnitName = 'E'\n",
    "fldgrn_folder = 'data/ProcData/FldGrnInfo'\n",
    "learner_default_dict = {} # To update it in the future. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_SubUnit = get_EmbeddingBlock_DataFlow(full_recfldgrn_list, default_SubUnitName)\n",
    "\n",
    "s = df_SubUnit.apply(lambda x: mapping_SubUnitName_to_SubUnitNNList(x['SubUnitName'], \n",
    "                                                                    x['input_names'],\n",
    "                                                                    default_BasicNNtype_To_NNName), \n",
    "                    axis = 1)\n",
    "df_SubUnit['SubUnit_BasicNN_List'] = s\n",
    "s = df_SubUnit.apply(lambda x: get_SubUnit_Default_NNPara_List(x['SubUnit_BasicNN_List'], \n",
    "                                                               x['input_names'],\n",
    "                                                               fldgrn_folder, \n",
    "                                                               learner_default_dict), axis = 1)\n",
    "\n",
    "df_SubUnit['SubUnit_DefaultBasicNN_List'] = s\n",
    "\n",
    "\n",
    "\n",
    "s = df_SubUnit.apply(lambda x: get_SubUnit_BasicNN_Config_List(x['SubUnit_BasicNN_List'], \n",
    "                                                               x['SubUnit_DefaultBasicNN_List'], \n",
    "                                                               x['input_names'], \n",
    "                                                               x['output_name'], \n",
    "                                                                embed_size, \n",
    "                                                                process, \n",
    "                                                               ), axis = 1)\n",
    "\n",
    "df_SubUnit['SubUnit_BasicNN_Config_List'] = s\n",
    "df_SubUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce199da2-8fc2-4a65-a76b-c0d2f5dca3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from fieldnn.module.embedblock import EmbedBlockLayer\n",
    "\n",
    "EmbedBlock = EmbedBlockLayer(df_SubUnit)\n",
    "\n",
    "RECFLD_TO_EMBEDTESNOR = EmbedBlock(RECFLD_TO_TENSOR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b9d8ab-7a27-4e5d-a39c-d260046a35a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-P@age-AgeNumeDft',\n",
       " 'B-P@basicInfo-basicInfoDft',\n",
       " 'B-P-EC@BasicInfo-BasicDft',\n",
       " 'B-P-EC@DT_min-DTDft',\n",
       " 'B-P-EC-A1C@DT-DTDft',\n",
       " 'B-P-EC-A1C@V-A1CNumeDft',\n",
       " 'B-P-EC-Diag@DT-DTDft',\n",
       " 'B-P-EC-Diag@Value-DiagDft',\n",
       " 'B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in RECFLD_TO_EMBEDTESNOR]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8412c749-070e-4227-9857-97bd243a9782",
   "metadata": {},
   "source": [
    "# Get Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd4a94c-55cb-40ab-afa3-c56c36b50169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-P@age-AgeNumeDft torch.Size([4, 19, 128])\n",
      "B-P@basicInfo-basicInfoDft torch.Size([4, 2, 128])\n",
      "B-P-EC@BasicInfo-BasicDft torch.Size([4, 25, 2, 128])\n",
      "B-P-EC@DT_min-DTDft torch.Size([4, 25, 7, 128])\n",
      "B-P-EC-A1C@DT-DTDft torch.Size([4, 25, 1, 7, 128])\n",
      "B-P-EC-A1C@V-A1CNumeDft torch.Size([4, 25, 1, 37, 128])\n",
      "B-P-EC-Diag@DT-DTDft torch.Size([4, 25, 22, 7, 128])\n",
      "B-P-EC-Diag@Value-DiagDft torch.Size([4, 25, 22, 3, 128])\n",
      "B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM torch.Size([4, 25, 1, 14, 121, 11, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for full_recfld, info_dict in RECFLD_TO_EMBEDTESNOR.items():\n",
    "    print(full_recfld, info_dict['info'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be4f167-ad4c-496c-ab26-ec56bd3c9dd2",
   "metadata": {},
   "source": [
    "# Learner: BasicNN Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06fab7c-f81f-474a-8b5e-211197959235",
   "metadata": {},
   "source": [
    "## TFM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07497e4f-e83a-43e2-9165-e4ca7c9ec391",
   "metadata": {},
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67c5a980-2f8b-4781-a61b-e24a6e5b4d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _addindent(s_, numSpaces):\n",
    "    s = s_.split('\\n')\n",
    "    # don't do anything for single-line stuff\n",
    "    if len(s) == 1:\n",
    "        return s_\n",
    "    first = s.pop(0)\n",
    "    s = [(numSpaces * ' ') + line for line in s]\n",
    "    s = '\\n'.join(s)\n",
    "    s = first + '\\n' + s\n",
    "    return s\n",
    "\n",
    "class TFMLayer(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size = 200, \n",
    "                 output_size = 200, # d_model\n",
    "                 nhead = 8,\n",
    "                 num_encoder_layers = 6, # only have encoder part\n",
    "                 num_decoder_layers = 0, # in default, we don't need decoder part. \n",
    "                 dim_feedforward = 2048, \n",
    "                 tfm_dropout = 0.1,\n",
    "                 tfm_activation = 'relu'):\n",
    "        \n",
    "        '''https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py'''\n",
    "\n",
    "        super(TFMLayer,self).__init__()\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.input_size = input_size\n",
    "        self.tfm_input_size = input_size\n",
    "        self.n_directions = 1\n",
    "        self.output_size = output_size\n",
    "        assert output_size % self.n_directions == 0 \n",
    "        self.hidden_size = int(output_size / self.n_directions)\n",
    "        assert self.hidden_size == self.tfm_input_size\n",
    "        # self.psn_size = psn_size \n",
    "        \n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        \n",
    "        self.transformer  = torch.nn.Transformer(d_model = self.hidden_size, \n",
    "                                                 nhead = nhead,\n",
    "                                                 num_encoder_layers = self.num_encoder_layers,\n",
    "                                                 num_decoder_layers = self.num_decoder_layers,\n",
    "                                                 dim_feedforward = dim_feedforward, \n",
    "                                                 dropout = tfm_dropout,\n",
    "                                                 activation = tfm_activation,\n",
    "                                                 batch_first = True,\n",
    "                                                 # src_mask_flag = False, # see all tokens in a sentence \n",
    "                                                 # # This IS THE NEW PART. NOT PyTorch.nn.\n",
    "                                                 ) \n",
    "\n",
    "\n",
    "    def forward(self, info, leng_mask):\n",
    "        info = self.transformer(info, info, \n",
    "                                src_key_padding_mask = leng_mask,  \n",
    "                                tgt_key_padding_mask = leng_mask)\n",
    "        # for layer in self.postprocess:\n",
    "        #     info = layer(info)\n",
    "        return info\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        # We treat the extra repr like the sub-module, one item per line\n",
    "        extra_lines = []\n",
    "        extra_repr = self.extra_repr()\n",
    "        # empty string will be split into list ['']\n",
    "        if extra_repr:\n",
    "            extra_lines = extra_repr.split('\\n')\n",
    "        child_lines = []\n",
    "        for key, module in self._modules.items():\n",
    "            mod_str = repr(module)\n",
    "            mod_str = _addindent(mod_str, 2)\n",
    "            child_lines.append('(' + key + '): ' + mod_str)\n",
    "        lines = extra_lines + child_lines\n",
    "\n",
    "        main_str = self._get_name() + f'LEARNER(TFM): input({self.input_size}), output({self.output_size})'\n",
    "        lines = [f'(Encoder): EncoderLayer(layers_num={self.num_encoder_layers}, dim_feedforward={self.dim_feedforward})', \n",
    "                 f'(Decoder): DecoderLayer(layers_num={self.num_decoder_layers}, dim_feedforward={self.dim_feedforward})']\n",
    "        main_str += '\\n  ' + '\\n  '.join(lines) + '\\n'\n",
    "        \n",
    "        # if lines:\n",
    "        #     # simple one-liner info, which most builtin Modules will use\n",
    "        #     if len(extra_lines) == 1 and not child_lines:\n",
    "        #         main_str += extra_lines[0]\n",
    "        #     else:\n",
    "        #         main_str += '\\n  ' + '\\n  '.join(lines) + '\\n'\n",
    "        # main_str += ')'\n",
    "        return main_str\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a2a576-9e0a-43d2-b7ba-3e7366f7d40c",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dda01b80-8c8d-4402-8a95-4bfe3990a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main Config\n",
    "### others will change with each recfldgrn\n",
    "\n",
    "#######################\n",
    "embed_size = 512\n",
    "default_tfm_para = {}\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa15c1dc-3fbf-4b60-91fd-ff16eb790a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tfm_para(embed_size, default_tfm_para):\n",
    "    assert 'input_size' not in default_tfm_para\n",
    "    \n",
    "    tfm_para =  {'input_size': embed_size,\n",
    "                 'output_size': embed_size,\n",
    "                 'nhead': 8,\n",
    "                 'num_encoder_layers': 6,\n",
    "                 'num_decoder_layers': 0,\n",
    "                 'dim_feedforward': 2048,\n",
    "                 'tfm_dropout': 0.1,\n",
    "                 'tfm_activation': 'relu'}\n",
    "    \n",
    "    for k, v in default_tfm_para: tfm_para[k] = v\n",
    "    return tfm_para\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d4b65-17e4-4273-b63e-470cec593a59",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e154d5-24d4-49e7-9b72-afb35144603f",
   "metadata": {},
   "source": [
    "**recfld information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f95cde1-19da-4184-b9a9-365d29414a79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-P-EC-A1C@V-A1CNumeDft torch.Size([4, 25, 1, 37, 128])\n",
      "B-P-EC-Diag@Value-DiagDft torch.Size([4, 25, 22, 3, 128])\n",
      "B-P-EC-PN-PNSect-PNSectSent@Sentence-Tk@TknzLLM torch.Size([4, 25, 1, 14, 121, 11, 128])\n"
     ]
    }
   ],
   "source": [
    "for recfld, EmbedTensor in full_recfldgrn_EmbedTensor.items():\n",
    "    print(recfld, EmbedTensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2e4ed-f241-4c9d-bb55-c5c8a018e69d",
   "metadata": {},
   "source": [
    "**get configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb4b00b-9981-4d0a-a6f1-c46ff8553c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b368c9c1-dde4-4caa-9284-c349ab129a80",
   "metadata": {},
   "source": [
    "**init model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74464e80-7e6c-4eb5-a610-42c4c7eb63e1",
   "metadata": {},
   "source": [
    "**prepare input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7c7e639-5962-46a9-9c55-57024b9bd560",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_layer = TFMLayer(**tfm_para)\n",
    "# tfm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "734071a3-8ded-408b-9001-a70d6c910984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFMLayerLEARNER(TFM): input(512), output(512)\n",
       "  (Encoder): EncoderLayer(layers_num=6, dim_feedforward=2048)\n",
       "  (Decoder): DecoderLayer(layers_num=0, dim_feedforward=2048)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483066b0-264f-474e-9962-8f53ba9c0d68",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28f87e-d545-498e-bdca-b7344eb3e542",
   "metadata": {},
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c12f30d1-6ce5-4e08-8a8c-0e1c6aedd3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_size  = 200, \n",
    "                 output_size = 200.\n",
    "                 ):\n",
    "\n",
    "        super(LinearLayer, self).__init__()\n",
    "    \n",
    "        self.input_size  = input_size\n",
    "        self.output_size = output_size\n",
    "        self.linear  = torch.nn.Linear(self.input_size, self.output_size)\n",
    "        self.init_weights()\n",
    "            \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, info):\n",
    "        info = self.linear(info)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16a34c-007f-47b2-9e0f-7ff022c6566d",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f20faf37-7ec0-4b2b-a344-7d73d7c9996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_para(embed_size, default_linear_para):\n",
    "    \n",
    "    linear_para =  {'input_size': embed_size,\n",
    "                    'output_size': embed_size}\n",
    "    \n",
    "    for k, v in default_linear_para: linear_para[k] = v\n",
    "    \n",
    "    return linear_para\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d026cbbd-1790-4c32-864d-1a5934ac7f40",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e8245-ae5a-468e-9fbc-e1bcce752aa9",
   "metadata": {},
   "source": [
    "**recfld information**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4c01f-006d-4013-a8f5-1faaa3eacdb2",
   "metadata": {},
   "source": [
    "**get configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284aaa8-3168-4580-a038-7ffafc001eb2",
   "metadata": {},
   "source": [
    "**init model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76799d6-28b4-4ac6-b5e2-0cc268e0cb73",
   "metadata": {},
   "source": [
    "**prepare input**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b7ec6c-9e54-489c-a658-83b77ccdf095",
   "metadata": {},
   "source": [
    "**I-NN-O**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b2b11-cc56-48ca-85ff-398d930e8618",
   "metadata": {},
   "source": [
    "# Learner: Basic NN Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb94409e-3a5b-4195-bb71-1a043db19a89",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b48f48-e1cf-4232-b2d9-aa1f2be6c516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from fieldnn.nn.tfm import TFMLayer\n",
    "from fieldnn.nn.linear import LinearLayer\n",
    "from fieldnn.utils.layerfn import orderSeq, restoreSeq, align_psn_idx, get_Layer2Holder\n",
    "from fieldnn.utils.parafn import generate_psn_embed_para\n",
    "\n",
    "\n",
    "class Learner_Layer(torch.nn.Module):\n",
    "    def __init__(self, input_fullname, output_fullname, learner_layer_para, PSN_Embed_ModuleDict):\n",
    "        super(Learner_Layer, self).__init__()\n",
    "        \n",
    "        # Part 0: Meta\n",
    "        self.fullname = input_fullname\n",
    "        \n",
    "        self.input_fullname = input_fullname\n",
    "        self.output_fullname = output_fullname\n",
    "        assert self.input_fullname == self.output_fullname\n",
    "        \n",
    "        self.input_size = learner_layer_para['input_size']\n",
    "        self.output_size = learner_layer_para['output_size']\n",
    "        self.embed_size = self.input_size\n",
    "        self.max_leng = learner_layer_para['max_leng']\n",
    "        \n",
    "            \n",
    "        # Part 1: NN\n",
    "        nn_name, para = learner_layer_para[self.fullname]\n",
    "        self.nn_name == nn_name\n",
    "        \n",
    "        if nn_name.lower() == 'tfm':\n",
    "            assert self.input_size == self.output_size\n",
    "            self.Learner = TFMLayer(**para)\n",
    "            # Part a: PSN embedding\n",
    "            self.psn_embedding = torch.nn.Embedding(self.max_leng + 1, self.input_size, padding_idx = 0)\n",
    "            # Part b: EmbedProcess\n",
    "            self.embedprocess = torch.nn.ModuleDict()\n",
    "            for method, config in learner_layer_para['embedprocess'].items():\n",
    "                if method == 'dropout':\n",
    "                    self.embedprocess[method] = torch.nn.Dropout(**config)\n",
    "                elif method == 'layernorm':\n",
    "                    self.embedprocess[method] = torch.nn.LayerNorm(self.output_size, **config)\n",
    "                else:\n",
    "                    raise ValueError(f'no avialable embedprocess method {method}')\n",
    "                    \n",
    "            self.forward = self.forward_tfm\n",
    "                \n",
    "        elif nn_name.lower() == 'linear':\n",
    "            self.Learner = LinearLayer(**para)\n",
    "            self.forward = self.forward_lnr\n",
    "            \n",
    "        # elif nn_name.lower() == 'cnn':\n",
    "        #     self.Learner = CNNLayer(**para)\n",
    "        \n",
    "        # elif nn_name.lower() == 'rnn':\n",
    "        #     self.Learner = RNNLayer(**para)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f'NN \"{nn_name}\" is not available')\n",
    "        \n",
    "        # Part 2: PostProcess\n",
    "        self.postprocess = torch.nn.ModuleDict()\n",
    "        for method, config in learner_layer_para['postprocess'].items():\n",
    "            if method == 'dropout':\n",
    "                self.postprocess[method] = torch.nn.Dropout(**config)\n",
    "            elif method == 'layernorm':\n",
    "                self.postprocess[method] = torch.nn.LayerNorm(self.output_size, **config)\n",
    "\n",
    "        self.Ignore_PSN_Layers = learner_layer_para['Ignore_PSN_Layers']\n",
    "        \n",
    "    # def get_psn_embed_tensor(self, fullname, holder):\n",
    "    #     name = fullname.split('-')[-1]\n",
    "    #     Layer2Idx = {v:idx for idx, v in enumerate(fullname.split('-'))}\n",
    "    #     Layer2Holder = get_Layer2Holder(fullname, holder, self.Ignore_PSN_Layers)\n",
    "    #     psn_embed = 0\n",
    "    #     for source_layer, Embed in self.PSN_Embed_ModuleDict.items():\n",
    "    #         cpsn_idx = align_psn_idx(source_layer, name, Layer2Idx, Layer2Holder)\n",
    "    #         psn_embed = psn_embed + Embed(cpsn_idx)\n",
    "    #     return psn_embed\n",
    "    \n",
    "    def reshape(self, info, leng_mask):\n",
    "        nbs = np.array(info.shape[:-2]).prod()\n",
    "        ngrn, dim = info.shape[-2:]\n",
    "        # print(nbs, ngrn, dim)\n",
    "        \n",
    "        tmp_info = info.contiguous().view(nbs, ngrn, dim)\n",
    "        # print(tmp_info.shape)\n",
    "\n",
    "        tmp_leng_mask = leng_mask.contiguous().view(nbs, ngrn)\n",
    "        # print(tmp_leng_mask.shape)\n",
    "\n",
    "        tmp_leng = (tmp_leng_mask == 0).sum(-1)\n",
    "        # print(tmp_leng.shape)\n",
    "        \n",
    "        ord_info,      ord_leng, r_idx = orderSeq(tmp_info, tmp_leng)\n",
    "        ord_leng_mask, ord_leng, r_idx = orderSeq(tmp_leng_mask, tmp_leng)\n",
    "        return ord_info, ord_leng_mask, r_idx\n",
    "    \n",
    "    def restore(self, ord_info_output, leng_mask, r_idx):\n",
    "        info_new = restoreSeq(ord_info_output, r_idx)\n",
    "        output_size = info_new.shape[-1]\n",
    "        info_output = info_new.view(*list(leng_mask.shape) + [output_size])\n",
    "        return info_output\n",
    "        \n",
    "    def generate_locidx(self, holder):\n",
    "        leng_mask = holder == 0\n",
    "        # leng = (leng_mask == 0).sum(-1)\n",
    "        psn_idx = (leng_mask == False).cumsum(-1).masked_fill(leng_mask, 0)\n",
    "        return psn_idx\n",
    "    \n",
    "    def forward_tfm(self, fullname, holder, info):\n",
    "        assert self.input_fullname == fullname\n",
    "        \n",
    "        # (1) adding psn embed\n",
    "        psn_locid = self.generate_locidx(holder) #### TODO\n",
    "        psn_embed = self.psn_embedding(psn_locid)\n",
    "        for nn, layer in self.embedprocess.items(): \n",
    "            psn_embed = layer(psn_embed)\n",
    "        info = info + psn_embed\n",
    "        # TODO: process psn_embed? Do we need the further embed process? \n",
    "        \n",
    "        # (2) do the tfm calculation\n",
    "        leng_mask = holder == 0\n",
    "        ord_info, ord_leng_mask, r_ix = self.reshape(info, leng_mask)\n",
    "        ord_info_output = self.Learner(ord_info, ord_leng_mask)\n",
    "        info = self.restore(ord_info_output, leng_mask, r_ix)\n",
    "        \n",
    "        # (3) post-process\n",
    "        for nn_name, layer in self.postprocess.items():\n",
    "            info = layer(info)\n",
    "            \n",
    "        # we do not change the fullname and holder\n",
    "        return self.output_fullname, holder, info\n",
    "    \n",
    "    \n",
    "    def forward_lnr(self, fullname, holder, info):\n",
    "        assert self.input_fullname == fullname\n",
    "        # (1) learn the info\n",
    "        info = self.Learner(info)\n",
    "        \n",
    "        # (2) do the masked_leng because of non-zero bias\n",
    "        leng_mask = holder == 0\n",
    "        info = info.masked_fill(leng_mask.unsqueeze(-1), 0)\n",
    "    \n",
    "        # (3) post-process\n",
    "        for nn_name, layer in self.postprocess.items():\n",
    "            info = layer(info)\n",
    "            \n",
    "        # we do not change the fullname and holder\n",
    "        return self.output_fullname, holder, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a976c1-42cc-4466-8be1-0cccf65418ae",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070a723-cdce-4c29-a557-38262e8eea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "\n",
    "def get_expander_para(nn_name, nn_para,\n",
    "                      embed_size, vocab_tokenizer, init, \n",
    "                      postprocess):\n",
    "    \n",
    "    expander_para = {}\n",
    "    \n",
    "    expander_para['nn_type'] = 'expander'  \n",
    "    expander_para['nn_name'] = nn_name\n",
    "\n",
    "    # (1) get the parameters\n",
    "\n",
    "    if nn_name.lower() == 'cateembed':\n",
    "        para = get_cateembed_para(embed_size, vocab_tokenizer, init)\n",
    "    elif nn_name.lower() == 'numeembed':\n",
    "        para = get_numeembed_para(embed_size, vocab_tokenizer, init)\n",
    "    elif nn_name.lower() == 'llmembed':\n",
    "        para = get_llmembed_para(embed_size,  vocab_tokenizer, init)\n",
    "    else:\n",
    "        raise ValueError(f'The NN \"{nn_name}\" is not available yet')\n",
    "        \n",
    "    expander_para['nn_para'] = para\n",
    "    \n",
    "    \n",
    "    #(2) Input size, output size\n",
    "    expander_para['input_size'] = None\n",
    "    expander_para['output_size'] = embed_size\n",
    "\n",
    "    # (3) Post Process\n",
    "    expander_para['postprocess'] = postprocess\n",
    "    \n",
    "    return expander_para"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb045c8-4c59-42b5-a521-db7e80065f97",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd950f8-ad27-4e9a-9f47-e05f45ec9322",
   "metadata": {},
   "source": [
    "### recfld informaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444ad58-9dd9-4a50-83f5-e5efd41a20d2",
   "metadata": {},
   "source": [
    "### get configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce20013-6062-4d10-9e5d-c20fbb4488a3",
   "metadata": {},
   "source": [
    "### init model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49295d3c-9abb-406a-872d-1642344f7e50",
   "metadata": {},
   "source": [
    "### prepare input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c7cf8-6a60-4324-a8d9-c2d0ec6aaedd",
   "metadata": {},
   "source": [
    "### I-NN-O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a341b4c-c21d-4f64-817a-9802b96be6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
