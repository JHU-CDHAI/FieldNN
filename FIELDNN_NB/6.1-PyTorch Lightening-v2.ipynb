{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee5d06d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/floydluo/Library/CloudStorage/GoogleDrive-jjluo@terpmail.umd.edu/My Drive/0-Research-Project/MedStar/MS_CODE/FieldNN\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc438e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightning'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import lightning.pytorch as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5ebb0",
   "metadata": {},
   "source": [
    "## Define a dataset and a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb27edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from recfldgrn.datapoint import convert_PID_to_PIDgroup\n",
    "from recfldgrn.datapoint import RANGE_SIZE, write_df_to_folders, load_df_data_from_folder\n",
    "from fieldnn.utils.layerfn import traverse, convert_relational_list_to_numpy\n",
    "\n",
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, TaskTensor_folder, recfldgrn_list):\n",
    "        self.recfldgrn_list = recfldgrn_list\n",
    "        self.TaskTensor_folder = TaskTensor_folder\n",
    "        \n",
    "        data = pd.DataFrame(columns = ['PID'])\n",
    "    \n",
    "        for recfldgrn in recfldgrn_list:\n",
    "    \n",
    "            # (1) get tensor_folder\n",
    "            tensor_folder = os.path.join(TaskTensor_folder, recfldgrn)\n",
    "\n",
    "            # (2) get df_Pat and full_recfldgrn\n",
    "            df_Pat = load_df_data_from_folder(tensor_folder)# .set_index('PID')\n",
    "            # print(df_Pat.col\n",
    "            data  = pd.merge(data, df_Pat, on = 'PID', how = 'right')\n",
    "\n",
    "        self.data = data\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data.iloc[index]# [full_recfldgrn]\n",
    "        \n",
    "        y = np.random.choice([0,1]) # go back to label later.\n",
    "        return x, y # torch.tensor(x), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0585badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(batch_input):\n",
    "    batch_rfg = {}\n",
    "    ##############\n",
    "    # inputs: you can check the following inputs in the above cells.\n",
    "    # (1): relational_list\n",
    "    # (2): new_full_recfldgrn\n",
    "    # (3): suffix\n",
    "    ##############\n",
    "    df_batch = pd.DataFrame([i[0].to_dict() for i in batch_input])\n",
    "    # df_batch\n",
    "    recfldgrn_list = [i for i in df_batch.columns if 'PID' not in i]\n",
    "    for full_recfldgrn in recfldgrn_list:\n",
    "        suffix = '_' + full_recfldgrn.split('_')[-1]\n",
    "        relational_list = df_batch[full_recfldgrn].to_list()\n",
    "\n",
    "        new_full_recfldgrn = 'B-' + full_recfldgrn # B- means Batch. \n",
    "        # output: this function will return a Dictionary to hold outcome\n",
    "        D = convert_relational_list_to_numpy(relational_list, new_full_recfldgrn, suffix)\n",
    "        # have a look at B-P-EC-A1C@DT-DTDftGrn_idx, the final tensor\n",
    "        tensor_idx = D[new_full_recfldgrn]\n",
    "        # print(new_full_recfldgrn, '<--- new_full_recfldgrn')\n",
    "        # print(tensor_idx.shape, '<------- the shape of tensor_idx')\n",
    "        \n",
    "        batch_rfg[new_full_recfldgrn] = torch.Tensor(tensor_idx)\n",
    "    ##############\n",
    "    \n",
    "    batch_y = torch.LongTensor([i[1] for i in batch_input])  # ignore this\n",
    "    return batch_rfg, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb0295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "TaskTensor_folder = 'data/ProcData/TensorFolder/Task2YearXXX'\n",
    "recfldgrn_list = [\n",
    "                  'P@age-AgeNumeDftGrn',\n",
    "                  'P@basicInfo-basicInfoDftGrn',\n",
    "    \n",
    "                  'EC@BasicInfo-BasicDftGrn',\n",
    "                  'EC@DT_min-DTDftGrn',\n",
    "    \n",
    "                  'A1C@DT-DTDftGrn',\n",
    "                  'A1C@V-A1CNumeDftGrn',\n",
    "                  \n",
    "                  'Diag@DT-DTDftGrn',\n",
    "                  'Diag@Value-DiagDftGrn',\n",
    "                  \n",
    "                  'PNSectSent@Sentence-Tk@TknzLLMGrn']\n",
    "\n",
    "\n",
    "train_dataset = PatientDataset(TaskTensor_folder, recfldgrn_list)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=my_collate_fn)\n",
    "\n",
    "# add valid_dataset and valid_dataloader\n",
    "# add test_dataset and test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b13df2f",
   "metadata": {},
   "source": [
    "## Embed and Repr Block Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe3f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fieldnn.dataflowfn.embedflowfn import get_EmbeddingBlock_SubUnit\n",
    "from fieldnn.dataflowfn.baseflowfn import mapping_SubUnitName_to_SubUnitNNList\n",
    "from fieldnn.dataflowfn.baseflowfn import get_SubUnit_Default_NNPara_List\n",
    "from fieldnn.dataflowfn.baseflowfn import get_SubUnit_BasicNN_Config_List\n",
    "from fieldnn.module.embedblock import EmbedBlockLayer\n",
    "\n",
    "\n",
    "from fieldnn.dataflowfn.reprflowfn import get_Repr_dataflow_table\n",
    "from fieldnn.dataflowfn.reprflowfn import update_df_Repr_dataflow\n",
    "from fieldnn.dataflowfn.reprflowfn import update_df_Repr_dataflow_completename\n",
    "from fieldnn.dataflowfn.reprflowfn import get_Repr_SubUnit_List\n",
    "\n",
    "from fieldnn.dataflowfn.baseflowfn import mapping_SubUnitName_to_SubUnitNNList\n",
    "from fieldnn.dataflowfn.baseflowfn import generate_BasicNN_Config\n",
    "from fieldnn.dataflowfn.baseflowfn import get_SubUnit_Default_NNPara_List\n",
    "from fieldnn.dataflowfn.baseflowfn import get_SubUnit_BasicNN_Config_List\n",
    "\n",
    "from fieldnn.module.reprblock import ReprBlockLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e25939",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "078016da",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = 'B-P'\n",
    "embed_size = 128\n",
    "output_size = 1\n",
    "actn_fn_name = 'Sigmoid' # torch.nn.Sigmoid()\n",
    "loss_fn_name = 'BCELoss' # torch.nn.BCELoss()\n",
    "fldgrn_folder = 'data/ProcData/FldGrnInfo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e820ac1d",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e33a55ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputBlock = torch.nn.Linear(embed_size, output_size)\n",
    "\n",
    "if actn_fn_name == 'Sigmoid':\n",
    "    actn_method = torch.nn.Sigmoid()\n",
    "    actn_fn = lambda outputvecs: actn_method(outputvecs) # will return probs \n",
    "elif self.actn_fn_name == 'Softmax':\n",
    "    actn_method = torch.nn.Softmax()\n",
    "    actn_fn = lambda outputvecs: actn_method(outputvecs, dim = 1) # will return probs \n",
    "else:\n",
    "    raise ValueError(f'Activation Function Name {actn_fn_name} is not available yet')\n",
    "    \n",
    "\n",
    "if loss_fn_name == 'BCELoss':\n",
    "    assert actn_fn_name == 'Sigmoid'\n",
    "    loss_method = torch.nn.BCELoss()\n",
    "    loss_fn = lambda probs, targets: loss_method(probs, targets) # will return loss\n",
    "elif self.loss_fn_name == 'CrossEntropyLoss':\n",
    "    assert actn_fn_name == 'Softmax'\n",
    "    loss_method = torch.nn.CrossEntropyLoss()\n",
    "    loss_fn = lambda probs, targets: loss_method(probs, targets) # will return loss\n",
    "else:\n",
    "    raise ValueError(f'Loss Function Name {loss_fn_name} is not available yet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdba308",
   "metadata": {},
   "source": [
    "## Define a LightningModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e4649",
   "metadata": {},
   "source": [
    "- The **training_step** defines how the nn.Modules interact together.\n",
    "\n",
    "- In the **configure_optimizers** define the optimizer(s) for your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0d26834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the PatientLevelPredictionModel\n",
    "class LitAutoModel(pl.LightningModule):\n",
    "    def __init__(self, fldgrn_folder,\n",
    "                 OutputBlock, actn_fn, loss_fn, \n",
    "                 output_name, embed_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_name = output_name\n",
    "        \n",
    "        self.OutputBlock = OutputBlock\n",
    "        self.actn_fn = actn_fn\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.output_size = output_size\n",
    "            \n",
    "        self.default_BasicNNtype_To_NNName = {\n",
    "            'expander': None,\n",
    "            'reducer': 'Max',\n",
    "            'merger': 'Merger',\n",
    "            'learner': None,\n",
    "        }\n",
    "        self.process = {'activator': 'gelu',\n",
    "           'dropout': {'p': 0.5, 'inplace': False},\n",
    "           'layernorm': {'eps': 1e-05, 'elementwise_affine': True}}\n",
    "        \n",
    "        self.default_E_subunit_name = 'E'\n",
    "        self.fldgrn_folder = fldgrn_folder\n",
    "        self.learner_default_dict = {}\n",
    "        self.default_R_subunit_name = 'R'\n",
    "        self.default_MR_subunit_name = 'MR'\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        \n",
    "        batch_rfg, y = batch\n",
    "        REPR_TENSOR = self.get_REPR_TENSOR(batch_rfg)\n",
    "        info_dict = REPR_TENSOR[self.output_name]\n",
    "        featvecs = info_dict['info']\n",
    "        \n",
    "        outputvecs = self.OutputBlock(featvecs)\n",
    "        probs = self.actn_fn(outputvecs)\n",
    "        \n",
    "        probs = probs.squeeze(1)\n",
    "        probs = probs.to(torch.float64)\n",
    "        y = y.to(torch.float64)\n",
    "        \n",
    "        loss = self.loss_fn(probs, y)\n",
    "        \n",
    "        print(\"loss: \", loss.detach().numpy())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        batch_rfg, y = batch\n",
    "        \n",
    "        REPR_TENSOR = self.get_REPR_TENSOR(batch_rfg)\n",
    "        info_dict = REPR_TENSOR[self.output_name]\n",
    "        featvecs = info_dict['info']\n",
    "        \n",
    "        outputvecs = self.OutputBlock(featvecs)\n",
    "        probs = self.actn_fn(outputvecs)\n",
    "        \n",
    "        probs = probs.squeeze(1)\n",
    "        probs = probs.to(torch.float64)\n",
    "        y = y.to(torch.float64)\n",
    "        \n",
    "        val_loss = self.loss_fn(probs, y)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        batch_rfg, y = batch\n",
    "        \n",
    "        REPR_TENSOR = self.get_REPR_TENSOR(batch_rfg)\n",
    "        info_dict = REPR_TENSOR[self.output_name]\n",
    "        featvecs = info_dict['info']\n",
    "        \n",
    "        outputvecs = self.OutputBlock(featvecs)\n",
    "        probs = self.actn_fn(outputvecs)\n",
    "        \n",
    "        probs = probs.squeeze(1)\n",
    "        probs = probs.to(torch.float64)\n",
    "        y = y.to(torch.float64)\n",
    "        \n",
    "        \n",
    "        test_loss = self.loss_fn(probs, y)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def get_REPR_TENSOR(self, batch_rfg):\n",
    "        # get the full_recfldgrn_list\n",
    "        full_recfldgrn_list = [i for i in batch_rfg]\n",
    "\n",
    "        # prepare RECFLD_TO_TENSOR\n",
    "        RECFLD_TO_TENSOR = {}\n",
    "        for full_recfldgrn in full_recfldgrn_list:\n",
    "            # (1) get the info_raw from batch_rfg\n",
    "            info_raw = batch_rfg[full_recfldgrn]\n",
    "\n",
    "            # (2) get the holder (input_idx) and holder_wgt (for nume embedding only)\n",
    "            if '_idx' in full_recfldgrn:\n",
    "                holder_wgt = 'Empty'\n",
    "                # holder = torch.LongTensor(info_raw)\n",
    "                holder = torch.tensor(info_raw).to(torch.int64)\n",
    "            elif '_wgt' in full_recfldgrn:\n",
    "                # holder_wgt = torch.FloatTensor(info_raw)\n",
    "                holder_wgt = torch.FloatTensor(info_raw)\n",
    "                # ATTENTION: here holder_wgt could contain zeros in some valid positions.\n",
    "                holder = torch.ones_like(holder_wgt).cumsum(-1).masked_fill(holder_wgt == 0, 0).long()\n",
    "            else:\n",
    "                raise ValueError(f'Invalid full_recfldgrn \"{full_recfldgrn}\"')\n",
    "\n",
    "            info_dict = {'holder': holder, 'holder_wgt': holder_wgt}\n",
    "            RECFLD_TO_TENSOR[full_recfldgrn] = info_dict\n",
    "    \n",
    "        \n",
    "        # get df_Embed_SubUnit \n",
    "        df_Embed_SubUnit = self.get_df_Embed_SubUnit(full_recfldgrn_list)\n",
    "        \n",
    "        # define EmbedBlock\n",
    "        self.EmbedBlock = EmbedBlockLayer(df_Embed_SubUnit)\n",
    "        \n",
    "        # get RECLD_TO_EMBEDTENSOR from RECFLD_TO_TENSOR and EmbedBlock\n",
    "        RECFLD_TO_EMBEDTESNOR = self.EmbedBlock(RECFLD_TO_TENSOR)\n",
    "        \n",
    "        # update the full_recfldgrn_list\n",
    "        full_recfldgrn_list = [i for i in RECFLD_TO_EMBEDTESNOR]\n",
    "\n",
    "        # get df_Repr_SubUnit \n",
    "        df_Repr_SubUnit = self.get_df_Repr_SubUnit(full_recfldgrn_list)\n",
    "        \n",
    "        # define ReprBlock\n",
    "        self.ReprBlock = ReprBlockLayer(df_Repr_SubUnit)\n",
    "        \n",
    "        # update the names of full_recfldgrn_list\n",
    "        fld_updates_dict = {}\n",
    "        for i in RECFLD_TO_EMBEDTESNOR:\n",
    "            layernum = len(i.split('-'))\n",
    "            fld = i.split('-')[-1]\n",
    "            if '@' not in fld: continue\n",
    "            \n",
    "            # print(fld)\n",
    "            neat_i = '-'.join(i.split('-')[:-1]) + '-' + fld.split('@')[0]\n",
    "            # print(neat_i)\n",
    "            same_neat_list = [t for t in RECFLD_TO_EMBEDTESNOR if neat_i + '@' in t]\n",
    "            # print(same_neat_list)\n",
    "            if len(same_neat_list) == 1: fld_updates_dict[i] = neat_i\n",
    "\n",
    "        for old, new in fld_updates_dict.items():\n",
    "            RECFLD_TO_EMBEDTESNOR[new] = RECFLD_TO_EMBEDTESNOR.pop(old)\n",
    "            \n",
    "        # get the OUTPUT_TO_TENSOR data holder\n",
    "        REPR_TENSOR = self.ReprBlock(RECFLD_TO_EMBEDTESNOR)\n",
    "        return REPR_TENSOR\n",
    "    \n",
    "    def get_df_Embed_SubUnit(self, full_recfldgrn_list):\n",
    "        df_Embed_SubUnit = get_EmbeddingBlock_SubUnit(full_recfldgrn_list, self.default_E_subunit_name)\n",
    "        s = df_Embed_SubUnit.apply(lambda x: mapping_SubUnitName_to_SubUnitNNList(x['SubUnitName'], \n",
    "                                                                            x['input_names'],\n",
    "                                                                            self.default_BasicNNtype_To_NNName), \n",
    "                                                                            axis = 1)\n",
    "        df_Embed_SubUnit['SubUnit_BasicNN_List'] = s\n",
    "        s = df_Embed_SubUnit.apply(lambda x: get_SubUnit_Default_NNPara_List(x['SubUnit_BasicNN_List'], \n",
    "                                                                       x['input_names'],\n",
    "                                                                       self.fldgrn_folder, \n",
    "                                                                       self.learner_default_dict), axis = 1)\n",
    "\n",
    "        df_Embed_SubUnit['SubUnit_DefaultBasicNN_List'] = s\n",
    "        s = df_Embed_SubUnit.apply(lambda x: get_SubUnit_BasicNN_Config_List(x['SubUnit_BasicNN_List'], \n",
    "                                                                       x['SubUnit_DefaultBasicNN_List'], \n",
    "                                                                       x['input_names'], \n",
    "                                                                       x['output_name'], \n",
    "                                                                        self.embed_size, \n",
    "                                                                        self.process, \n",
    "                                                                       ), axis = 1)\n",
    "        df_Embed_SubUnit['SubUnit_BasicNN_Config_List'] = s\n",
    "        \n",
    "        return df_Embed_SubUnit\n",
    "    \n",
    "    def get_df_Repr_SubUnit(self, full_recfldgrn_list):\n",
    "        \n",
    "        df_dataflow = get_Repr_dataflow_table(full_recfldgrn_list)\n",
    "        \n",
    "        df_dataflow_new = update_df_Repr_dataflow(df_dataflow, style = 'Reducer&Merger')\n",
    "        df_dataflow = df_dataflow_new.copy()\n",
    "        \n",
    "        df_Repr_SubUnit = get_Repr_SubUnit_List(df_dataflow, self.default_R_subunit_name, self.default_MR_subunit_name)\n",
    "        \n",
    "        s = df_Repr_SubUnit.apply(lambda x: mapping_SubUnitName_to_SubUnitNNList(x['SubUnitName'], \n",
    "                                                                    x['input_names'],\n",
    "                                                                    self.default_BasicNNtype_To_NNName), \n",
    "                                                                    axis = 1)\n",
    "\n",
    "        df_Repr_SubUnit['SubUnit_BasicNN_List'] = s\n",
    "\n",
    "        s = df_Repr_SubUnit.apply(lambda x: get_SubUnit_Default_NNPara_List(x['SubUnit_BasicNN_List'], \n",
    "                                                               x['input_names'],\n",
    "                                                               self.fldgrn_folder, \n",
    "                                                               self.learner_default_dict), axis = 1)\n",
    "\n",
    "        df_Repr_SubUnit['SubUnit_DefaultBasicNN_List'] = s\n",
    "        \n",
    "        s = df_Repr_SubUnit.apply(lambda x: get_SubUnit_BasicNN_Config_List(x['SubUnit_BasicNN_List'], \n",
    "                                                               x['SubUnit_DefaultBasicNN_List'], \n",
    "                                                               x['input_names'], \n",
    "                                                               x['output_name'], \n",
    "                                                                self.embed_size, \n",
    "                                                                self.process, \n",
    "                                                               ), axis = 1)\n",
    "\n",
    "        df_Repr_SubUnit['SubUnit_BasicNN_Config_List'] = s\n",
    "        \n",
    "        return df_Repr_SubUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3e14f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the LitAutoModel\n",
    "PatientLevelPredictionModel = LitAutoModel(fldgrn_folder, OutputBlock, actn_fn, loss_fn,\n",
    "                                    output_name, embed_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f546909",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "22f63c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "num_gpus = 0\n",
    "checkpoint_path = 'checkpoint/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6a6c3a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/joanna/opt/anaconda3/envs/CDHAI/lib/python3.8/site-packages/lightning/pytorch/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/Users/joanna/opt/anaconda3/envs/CDHAI/lib/python3.8/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory checkpoint/lightning_logs/version_0/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name        | Type   | Params\n",
      "---------------------------------------\n",
      "0 | OutputBlock | Linear | 129   \n",
      "---------------------------------------\n",
      "129       Trainable params\n",
      "0         Non-trainable params\n",
      "129       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "/Users/joanna/opt/anaconda3/envs/CDHAI/lib/python3.8/site-packages/lightning/fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory checkpoint/lightning_logs/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "/Users/joanna/opt/anaconda3/envs/CDHAI/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/joanna/opt/anaconda3/envs/CDHAI/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/4g5nwvg50r542_pn_sphckxr0000gn/T/ipykernel_32043/8362718.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  holder = torch.tensor(info_raw).to(torch.int64)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.8641, dtype=torch.float64, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 0:  25%|██████▊                    | 1/4 [00:18<00:54, 18.28s/it, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/4g5nwvg50r542_pn_sphckxr0000gn/T/ipykernel_32043/8362718.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  holder = torch.tensor(info_raw).to(torch.int64)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.9713, dtype=torch.float64, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 0:  50%|█████████████▌             | 2/4 [01:18<01:18, 39.09s/it, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/4g5nwvg50r542_pn_sphckxr0000gn/T/ipykernel_32043/8362718.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  holder = torch.tensor(info_raw).to(torch.int64)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.8551, dtype=torch.float64, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 0:  75%|████████████████████▎      | 3/4 [01:46<00:35, 35.64s/it, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/4g5nwvg50r542_pn_sphckxr0000gn/T/ipykernel_32043/8362718.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  holder = torch.tensor(info_raw).to(torch.int64)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.6858, dtype=torch.float64, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 1:   0%|                                   | 0/4 [00:00<?, ?it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/4g5nwvg50r542_pn_sphckxr0000gn/T/ipykernel_32043/8362718.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  holder = torch.tensor(info_raw).to(torch.int64)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.8227, dtype=torch.float64, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 1:  25%|██████▊                    | 1/4 [00:27<01:21, 27.17s/it, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/4g5nwvg50r542_pn_sphckxr0000gn/T/ipykernel_32043/8362718.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  holder = torch.tensor(info_raw).to(torch.int64)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.9038, dtype=torch.float64, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 1:  50%|█████████████▌             | 2/4 [01:23<01:23, 41.72s/it, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/4g5nwvg50r542_pn_sphckxr0000gn/T/ipykernel_32043/8362718.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  holder = torch.tensor(info_raw).to(torch.int64)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.7308, dtype=torch.float64, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 1:  75%|████████████████████▎      | 3/4 [02:08<00:42, 42.84s/it, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/4g5nwvg50r542_pn_sphckxr0000gn/T/ipykernel_32043/8362718.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  holder = torch.tensor(info_raw).to(torch.int64)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.6616, dtype=torch.float64, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch 1: 100%|███████████████████████████| 4/4 [02:47<00:00, 41.89s/it, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████████████| 4/4 [02:48<00:00, 42.10s/it, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=num_epochs, default_root_dir=checkpoint_path)\n",
    "trainer.fit(model=PatientLevelPredictionModel, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff68a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with validation set\n",
    "# Train the model\n",
    "trainer.fit(model=PatientLevelPredictionModel, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce8311",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9cafb249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joanna/opt/anaconda3/envs/CDHAI/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/Users/joanna/opt/anaconda3/envs/CDHAI/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|                               | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/4g5nwvg50r542_pn_sphckxr0000gn/T/ipykernel_32043/8362718.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  holder = torch.tensor(info_raw).to(torch.int64)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  25%|█████▊                 | 1/4 [00:17<00:53, 17.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/4g5nwvg50r542_pn_sphckxr0000gn/T/ipykernel_32043/8362718.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  holder = torch.tensor(info_raw).to(torch.int64)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  50%|███████████▌           | 2/4 [00:23<00:23, 11.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/4g5nwvg50r542_pn_sphckxr0000gn/T/ipykernel_32043/8362718.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  holder = torch.tensor(info_raw).to(torch.int64)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  75%|█████████████████▎     | 3/4 [00:36<00:12, 12.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/4g5nwvg50r542_pn_sphckxr0000gn/T/ipykernel_32043/8362718.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  holder = torch.tensor(info_raw).to(torch.int64)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|███████████████████████| 4/4 [00:48<00:00, 12.06s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.6864618353303682\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.6864618353303682}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the model\n",
    "# trainer.test(model, dataloaders=DataLoader(test_set))\n",
    "\n",
    "trainer.test(model=PatientLevelPredictionModel, dataloaders=train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
