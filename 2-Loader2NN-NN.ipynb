{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47a2660-824c-41fd-81e5-58a2207cbc74",
   "metadata": {},
   "source": [
    "# Simulate Data\n",
    "\n",
    "\n",
    "Given a field name, and first 2 dimensional numbes, simulate tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c7d713-7310-40cb-86f3-358ecf3274bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fieldnn.utils.layerfn import traverse\n",
    "from fieldnn.utils.simulate import get_next_info, get_simulated_tensor_from_fldname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5bf74-00e1-41c6-ab91-f25613356b2d",
   "metadata": {},
   "source": [
    "# Holder\n",
    "\n",
    "1. Current Layer:\n",
    "```python\n",
    "leng_mask = info_idx == 0 # or info_idx != 0\n",
    "leng = leng_mask.sum(-1)\n",
    "```\n",
    "\n",
    "2. Transfer\n",
    "\n",
    "```python\n",
    "old_leng = leng\n",
    "```\n",
    "\n",
    "\n",
    "3. Next Layer\n",
    "```python\n",
    "leng_mask = old_leng != 0\n",
    "leng = (leng_mask == 0).sum(-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84cbfc6e-dd63-459a-ad83-4c5d940788f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# leng_mask = info_idx == 0\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_Layer2Holder(fullname, holder, Ignore_PSN_Layers = ['B', 'P']):\n",
    "    # holder = holder\n",
    "    d = {}\n",
    "    for layername in list(reversed(fullname.split('-'))):\n",
    "        if layername in Ignore_PSN_Layers: continue\n",
    "        leng_mask = holder == 0\n",
    "        leng = (leng_mask == 0).sum(-1)\n",
    "        psn_idx = (leng_mask == False).cumsum(-1).masked_fill(leng_mask, 0)\n",
    "        d[layername] = {'holder': holder, \n",
    "                        'leng_mask': leng_mask, \n",
    "                        'leng': leng, \n",
    "                        'psn_idx': psn_idx}\n",
    "        # d[layername] = holder, psn_idx\n",
    "        holder = leng\n",
    "    Layer2Hoder = d\n",
    "    return Layer2Hoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee61bb27-4e3f-4dc8-8ba2-d7a4772acf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_psn_idx(source_layer, current_layer, Layer2Idx, Layer2Holder):\n",
    "    if source_layer == current_layer:\n",
    "        psn_idx = Layer2Holder[current_layer]['psn_idx']\n",
    "        return psn_idx\n",
    "    else:\n",
    "        source_psn_idx = Layer2Holder[source_layer]['psn_idx']\n",
    "        current_leng_mask = Layer2Holder[current_layer]['leng_mask']\n",
    "        gaps = Layer2Idx[current_layer] - Layer2Idx[source_layer]\n",
    "        # print(gaps)\n",
    "        # print(layername)\n",
    "        # print(prev_info.shape)\n",
    "        # print(leng_mask.shape)\n",
    "        # print(leng.shape)\n",
    "        # print(psn_idx.shape)\n",
    "        shape0 = list(source_psn_idx.shape) + [1] * gaps\n",
    "        shape1 = current_leng_mask.shape\n",
    "        psn_idx = source_psn_idx.view(*shape0).expand(shape1).masked_fill(current_leng_mask, 0)\n",
    "        # print(cpsn_idx.shape)\n",
    "        return psn_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb41993-c83b-4019-b270-63a024e32188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1 --> (3,)\n",
      "2 --> (3, 6)\n",
      "2\n",
      "2 --> (3, 6)\n",
      "3 --> (3, 6, 3)\n",
      "3\n",
      "3 --> (3, 6, 3)\n",
      "4 --> (3, 6, 3, 3)\n",
      "(3, 6, 3, 3)\n",
      "{'B': 0, 'PatRec:EC': 1, 'ECRec:Diag': 2, 'DiagRec:DiagV': 3, 'DiagVdftGrn': 4}\n",
      "DiagVdftGrn\n",
      "['DiagVdftGrn', 'DiagRec:DiagV', 'ECRec:Diag', 'PatRec:EC'] <---- Layer2Holder\n",
      "['DiagVdftGrn', 'DiagRec:DiagV', 'ECRec:Diag', 'PatRec:EC'] <---- psn_layers\n",
      "DiagVdftGrn torch.Size([3, 6, 3, 3])\n",
      "DiagRec:DiagV torch.Size([3, 6, 3, 3])\n",
      "ECRec:Diag torch.Size([3, 6, 3, 3])\n",
      "PatRec:EC torch.Size([3, 6, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# ======= within forward\n",
    "\n",
    "###############\n",
    "B_lenP = 3\n",
    "B2P_lnEC = [6, 5, 2] # \n",
    "prefix_layers_num = 2\n",
    "vocab_size = 5001\n",
    "Ignore_PSN_Layers = ['B', 'P']\n",
    "###############\n",
    "\n",
    "\n",
    "fullname = 'B-PatRec:EC-ECRec:Diag-DiagRec:DiagV-DiagVdftGrn'\n",
    "info_idx = get_simulated_tensor_from_fldname(fullname, B_lenP, B2P_lnEC, prefix_layers_num, vocab_size)\n",
    "print(info_idx.shape)\n",
    "holder = torch.LongTensor(info_idx)\n",
    "\n",
    "\n",
    "############### gsn_embeddings\n",
    "Layer2Idx = {v:idx for idx, v in enumerate(fullname.split('-'))}\n",
    "name = fullname.split('-')[-1]\n",
    "Layer2Holder = get_Layer2Holder(fullname, holder, Ignore_PSN_Layers)\n",
    "psn_layers = list(reversed([i for i in Layer2Idx if i not in Ignore_PSN_Layers]))\n",
    "\n",
    "\n",
    "print(Layer2Idx)\n",
    "print(name)\n",
    "print([i for i in Layer2Holder], '<---- Layer2Holder')\n",
    "print(psn_layers, '<---- psn_layers')\n",
    "\n",
    "for source_layer in psn_layers:\n",
    "    cpsn_idx = align_psn_idx(source_layer, name, Layer2Idx, Layer2Holder)\n",
    "    print(source_layer, cpsn_idx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46d1e8-572e-4144-9cea-07f7ffa37024",
   "metadata": {},
   "source": [
    "# Embedding (Expander)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efbdd72-00cf-4032-a9fe-f03dbe0d21a4",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "734f6b6b-7a1e-443e-b5fb-5aa3cab8a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fieldlm.nn.embedding\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class EmbeddingLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 embedding_size, \n",
    "                 init = 'init', \n",
    "                 freeze = False):\n",
    "        \n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        \n",
    "        # (+) self.embedding\n",
    "        if type(init) == np.ndarray:\n",
    "            # 1. from given array\n",
    "            weight = torch.FloatTensor(init)\n",
    "            assert weight.shape == (input_size, embedding_size)\n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(weight, freeze = freeze)\n",
    "            \n",
    "        elif os.path.isfile(init):\n",
    "            weight = torch.FloatTensor(np.load(init))\n",
    "            assert tuple(weight.shape) == (input_size, embedding_size)\n",
    "            \n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(weight, freeze = freeze)\n",
    "            \n",
    "        else:\n",
    "            # from random initialization\n",
    "            self.embedding = torch.nn.Embedding(input_size, embedding_size, padding_idx = 0)\n",
    "        \n",
    "    def forward(self, info):\n",
    "        # tensor0 to tensor1\n",
    "        info = self.embedding(info)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c037187-afc8-488f-a05b-ea6e63f0bc2c",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba78e46-aed6-47a9-aad5-1388e3324630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DiagVdftGrn', 'DiagRec:DiagV', 'ECRec:Diag', 'PatRec:EC']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in Layer2Holder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bfd2eb4-a95d-4188-96ae-ff8ded732878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'holder': tensor([[2, 1, 3, 3, 2, 3],\n",
       "         [2, 1, 3, 1, 1, 0],\n",
       "         [3, 2, 0, 0, 0, 0]]),\n",
       " 'leng_mask': tensor([[False, False, False, False, False, False],\n",
       "         [False, False, False, False, False,  True],\n",
       "         [False, False,  True,  True,  True,  True]]),\n",
       " 'leng': tensor([6, 5, 2]),\n",
       " 'psn_idx': tensor([[1, 2, 3, 4, 5, 6],\n",
       "         [1, 2, 3, 4, 5, 0],\n",
       "         [1, 2, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Layer2Holder['ECRec:Diag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dbeeb54-4aa5-4947-8d72-6ba9567b3cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding_size': 512, 'init': 'random', 'input_size': 5001}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 512\n",
    "vocab_size = 5000\n",
    "\n",
    "embed_para =  {'embedding_size': embed_size,\n",
    "               'init': 'random', \n",
    "               'input_size': vocab_size + 1 } # 1:the size of special tokens\n",
    "embed_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f877e4d8-fcd2-41fe-a59d-71afa84765c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_psn_embed_para(layername, embed_size):\n",
    "    \n",
    "    if 'Grn' not in layername: \n",
    "        vocab_size = 100\n",
    "    else:\n",
    "        vocab_size = 512\n",
    "        \n",
    "    embed_para = {'embedding_size': embed_size,\n",
    "                  'init': 'random', \n",
    "                  'input_size': vocab_size + 1 }\n",
    "    return embed_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba04c59f-9794-4410-a8c7-86016b00d3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-PatRec:EC-ECRec:Diag-DiagRec:DiagV-DiagVdftGrn': {'embedding_size': 512,\n",
       "  'init': 'random',\n",
       "  'input_size': 5001},\n",
       " 'DiagVdftGrn_psn': {'embedding_size': 512,\n",
       "  'init': 'random',\n",
       "  'input_size': 513},\n",
       " 'DiagRec:DiagV_psn': {'embedding_size': 512,\n",
       "  'init': 'random',\n",
       "  'input_size': 101},\n",
       " 'ECRec:Diag_psn': {'embedding_size': 512,\n",
       "  'init': 'random',\n",
       "  'input_size': 101},\n",
       " 'PatRec:EC_psn': {'embedding_size': 512, 'init': 'random', 'input_size': 101}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "d[fullname] = embed_para\n",
    "\n",
    "\n",
    "\n",
    "psn_layers = [i for i in Layer2Holder]\n",
    "\n",
    "for layername in psn_layers:\n",
    "    if layername == 'P': break\n",
    "    embed_para = generate_psn_embed_para(layername, embed_size)\n",
    "    d[f'{layername}_psn'] = generate_psn_embed_para(layername, embed_size)\n",
    "    \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "048332fb-6c14-401a-8886-49e1400b28f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-PatRec:EC-ECRec:Diag-DiagRec:DiagV-DiagVdftGrn\n",
      "{'embedding_size': 512, 'init': 'random', 'input_size': 5001}\n",
      "DiagVdftGrn_psn\n",
      "{'embedding_size': 512, 'init': 'random', 'input_size': 513}\n",
      "DiagRec:DiagV_psn\n",
      "{'embedding_size': 512, 'init': 'random', 'input_size': 101}\n",
      "ECRec:Diag_psn\n",
      "{'embedding_size': 512, 'init': 'random', 'input_size': 101}\n",
      "PatRec:EC_psn\n",
      "{'embedding_size': 512, 'init': 'random', 'input_size': 101}\n"
     ]
    }
   ],
   "source": [
    "for nn_name, layer_para in d.items():\n",
    "    print(nn_name)\n",
    "    print(layer_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e9c31a-8e65-41a1-ba35-9db4d56cf9d4",
   "metadata": {},
   "source": [
    "## Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d27962a-d6d3-4d44-80f5-09bf341447fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-PatRec:EC-ECRec:Diag-DiagRec:DiagV-DiagVdftGrn': EmbeddingLayer(\n",
       "   (embedding): Embedding(5001, 512, padding_idx=0)\n",
       " ),\n",
       " 'DiagVdftGrn_psn': EmbeddingLayer(\n",
       "   (embedding): Embedding(513, 512, padding_idx=0)\n",
       " ),\n",
       " 'DiagRec:DiagV_psn': EmbeddingLayer(\n",
       "   (embedding): Embedding(101, 512, padding_idx=0)\n",
       " ),\n",
       " 'ECRec:Diag_psn': EmbeddingLayer(\n",
       "   (embedding): Embedding(101, 512, padding_idx=0)\n",
       " ),\n",
       " 'PatRec:EC_psn': EmbeddingLayer(\n",
       "   (embedding): Embedding(101, 512, padding_idx=0)\n",
       " )}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_Dict = {}\n",
    "\n",
    "for nn_name, layer_para in d.items():\n",
    "    layer = EmbeddingLayer(**layer_para)\n",
    "    NN_Dict[nn_name] = layer\n",
    "    \n",
    "NN_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0616728-fd99-4536-81f4-bc21a9820636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1 --> (3,)\n",
      "2 --> (3, 6)\n",
      "2\n",
      "2 --> (3, 6)\n",
      "3 --> (3, 6, 4)\n",
      "3\n",
      "3 --> (3, 6, 4)\n",
      "4 --> (3, 6, 4, 4)\n",
      "(3, 6, 4, 4)\n",
      "torch.Size([3, 6, 4, 4])\n",
      "DiagVdftGrn\n",
      "DiagRec:DiagV\n",
      "ECRec:Diag\n",
      "PatRec:EC\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "########################\n",
    "B_lenP = 3\n",
    "B2P_lnEC = [6, 4, 3] # \n",
    "prefix_layers_num = 2\n",
    "vocab_size = 100\n",
    "########################\n",
    "\n",
    "fullname = 'B-PatRec:EC-ECRec:Diag-DiagRec:DiagV-DiagVdftGrn'\n",
    "layer2layeridx = {v:idx for idx, v in enumerate(fullname.split('2'))}\n",
    "name = fullname.split('-')[-1]\n",
    "\n",
    "data = get_simulated_tensor_from_fldname(fullname, B_lenP, B2P_lnEC, prefix_layers_num, vocab_size)\n",
    "print(data.shape)\n",
    "# fld_tensor_idx\n",
    "\n",
    "\n",
    "info_idx = torch.LongTensor(data)\n",
    "print(info_idx.shape)\n",
    "holder = info_idx\n",
    "# grn_leng_mask = info_idx == 0\n",
    "# print(grn_leng_mask.shape)\n",
    "\n",
    "Layer2Holder = get_Layer2Holder(fullname, holder)\n",
    "for i in Layer2Holder: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b12ed771-69e2-447f-b899-8c205a4ac9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingLayer(\n",
       "  (embedding): Embedding(5001, 512, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed\n",
    "Embed = NN_Dict[fullname]\n",
    "Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e87e0bf7-be84-4a02-9d00-b1e30fd55ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6, 4, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "info = Embed(info_idx)\n",
    "# tensor_name = tensor_name.replace('_idx', '2Feat_flt')\n",
    "print(info.shape)\n",
    "# print(tensor_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d5310dd-6982-4346-9e80-69a61e22ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_name.replace('_idx', '2Feat_flt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3b98f4-6655-4341-994f-e2b565a96d5c",
   "metadata": {},
   "source": [
    "# LMEmbeding (Expander)\n",
    "\n",
    "TODO: adding huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2efc1-c2d7-4add-a485-cb7f64e94f9e",
   "metadata": {},
   "source": [
    "# Transformer (Learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e179734-2f05-4220-b842-e7331d946abe",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4502f140-697c-412e-9709-17f5bc719339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TFMLayer(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size = 512, \n",
    "                 output_size = 512, # d_model\n",
    "                 nhead = 8,\n",
    "                 num_encoder_layers = 6, # only have encoder part\n",
    "                 num_decoder_layers = 0, # in default, we don't need decoder part. \n",
    "                 dim_feedforward = 2048, \n",
    "                 tfm_dropout = 0.1,\n",
    "                 tfm_activation = 'relu'):\n",
    "        \n",
    "        '''https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py'''\n",
    "\n",
    "        super(TFMLayer,self).__init__()\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.input_size = input_size\n",
    "        self.tfm_input_size = input_size\n",
    "        self.n_directions = 1\n",
    "        self.output_size = output_size\n",
    "        assert output_size % self.n_directions == 0 \n",
    "        self.hidden_size = int(output_size / self.n_directions)\n",
    "        assert self.hidden_size == self.tfm_input_size\n",
    "            \n",
    "        self.transformer  = torch.nn.Transformer(d_model = self.hidden_size, \n",
    "                                                 nhead = nhead,\n",
    "                                                 num_encoder_layers = self.num_encoder_layers,\n",
    "                                                 num_decoder_layers = self.num_decoder_layers,\n",
    "                                                 dim_feedforward = dim_feedforward, \n",
    "                                                 dropout = tfm_dropout,\n",
    "                                                 activation = tfm_activation,\n",
    "                                                 batch_first = True,\n",
    "                                                 # src_mask_flag = False, # see all tokens in a sentence \n",
    "                                                 # # This IS THE NEW PART. NOT PyTorch.nn.\n",
    "                                                 ) \n",
    "\n",
    "\n",
    "    def forward(self, info, leng_mask):\n",
    "        info = self.transformer(info, info, src_key_padding_mask = leng_mask,  tgt_key_padding_mask  = leng_mask)\n",
    "        # for layer in self.postprocess:\n",
    "        #     info = layer(info)\n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0a55c-ce55-411d-ab7d-058e2f5d3dbd",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df7319aa-ed3f-4d76-8dd6-ca0db4c14fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': 512,\n",
       " 'output_size': 512,\n",
       " 'nhead': 8,\n",
       " 'num_encoder_layers': 6,\n",
       " 'num_decoder_layers': 0,\n",
       " 'dim_feedforward': 2048,\n",
       " 'tfm_dropout': 0.1,\n",
       " 'tfm_activation': 'relu'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm_para =  {'input_size': 512,\n",
    "             'output_size': 512,\n",
    "             'nhead': 8,\n",
    "             'num_encoder_layers': 6,\n",
    "             'num_decoder_layers': 0,\n",
    "             'dim_feedforward': 2048,\n",
    "             'tfm_dropout': 0.1,\n",
    "             'tfm_activation': 'relu'}\n",
    "tfm_para"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee79c2-a9c3-4282-93d8-8ad6737485a1",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "803f5e37-6389-4280-933c-89bf734b48ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFMLayer(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList()\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm_layer = TFMLayer(**tfm_para)\n",
    "tfm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8c6aaaf-ffdd-48a6-b708-19b66d5d9dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-PatRec:EC-ECRec:Diag-DiagRec:DiagV-DiagVdftGrn\n",
      "torch.Size([3, 6, 4, 4])\n",
      "torch.Size([3, 6, 4, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "print(fullname)\n",
    "print(info_idx.shape)\n",
    "info = Embed(info_idx)\n",
    "print(info.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d94b742-2dd4-4c8e-b267-15049186be5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6, 4, 4])\n",
      "torch.Size([3, 6, 4, 4, 512])\n",
      "torch.Size([3, 6, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(info_idx.shape)\n",
    "leng_mask = info_idx == 0\n",
    "print(info.shape)\n",
    "print(leng_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c64af-93c2-4847-9524-d22dfd7ff2b6",
   "metadata": {},
   "source": [
    "### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09298b82-285d-4a21-ad72-cc2fbd8c87f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 4 512\n",
      "torch.Size([72, 4, 512])\n",
      "torch.Size([72, 4])\n",
      "torch.Size([72])\n"
     ]
    }
   ],
   "source": [
    "nbs = np.array(info.shape[:-2]).prod()\n",
    "ngrn, dim = info.shape[-2:]\n",
    "print(nbs, ngrn, dim)\n",
    "\n",
    "\n",
    "tmp_info = info.contiguous().view(nbs, ngrn, dim)\n",
    "print(tmp_info.shape)\n",
    "\n",
    "tmp_leng_mask = leng_mask.contiguous().view(nbs, ngrn)\n",
    "print(tmp_leng_mask.shape)\n",
    "\n",
    "tmp_leng = (tmp_leng_mask == 0).sum(-1)\n",
    "print(tmp_leng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48947be8-20b4-460c-b140-31af3af45cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order sequences and restore sequences according to their lenghts\n",
    "# TODO: test the speed of orderSeq and restoreSeq\n",
    "\n",
    "def orderSeq(seq_unordered, leng_unordered):\n",
    "    # leng_unordered is a tensor\n",
    "    # seq_unordered is a numpy\n",
    "    leng_ordered, seq_index = leng_unordered.sort(descending=True) \n",
    "    _, reverse_index = seq_index.sort()\n",
    "    leng_ordered = leng_ordered[leng_ordered>0]\n",
    "    seq_index    = seq_index[:len(leng_ordered)]\n",
    "    seq_ordered  = seq_unordered[seq_index.cpu()]\n",
    "    return seq_ordered, leng_ordered, reverse_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c29c883-44dd-49d6-bf11-dc60e2f8c3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 4, 512])\n",
      "torch.Size([32, 4, 512])\n",
      "torch.Size([32])\n",
      "torch.Size([72])\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "tmp_info = info.contiguous().view(nbs, ngrn, dim)\n",
    "print(tmp_info.shape)\n",
    "\n",
    "ord_info,      ord_leng, r_idx = orderSeq(tmp_info, tmp_leng)\n",
    "ord_leng_mask, ord_leng, r_idx = orderSeq(tmp_leng_mask, tmp_leng)\n",
    "print(ord_info.shape)\n",
    "print(ord_leng.shape)\n",
    "print(r_idx.shape)\n",
    "print(ord_leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f04a0a8-4e3f-4fdc-9ca6-9e2fe8773df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 4])\n",
      "torch.Size([32, 4])\n",
      "torch.Size([32])\n",
      "torch.Size([72])\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(tmp_leng_mask.shape)\n",
    "ord_leng_mask, ord_leng, r_idx = orderSeq(tmp_leng_mask, tmp_leng)\n",
    "print(ord_leng_mask.shape)\n",
    "print(ord_leng.shape)\n",
    "print(r_idx.shape)\n",
    "print(ord_leng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6d15a-edbc-408c-be52-92d08eaf6069",
   "metadata": {},
   "source": [
    "### Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "180ec929-b20b-4b8d-8aed-2d5d39975bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "ord_info_output = tfm_layer(ord_info, ord_leng_mask)\n",
    "print(ord_info_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce140a9-1cd8-4c58-a05a-7e62b0fe1407",
   "metadata": {},
   "source": [
    "### Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "685e4ec6-d06f-4ae8-a0ec-796732b1a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restoreSeq(seq_ordered, reverse_index):\n",
    "    # shape = list(seq_ordered.shape)\n",
    "    data_type = seq_ordered.type()\n",
    "    shape = list(seq_ordered.shape)\n",
    "    shape[0] = len(reverse_index) - shape[0]\n",
    "    t = torch.cat([seq_ordered, torch.zeros(shape).type(data_type)])\n",
    "    seq_restored = t[reverse_index]\n",
    "    return seq_restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6c3be77-0194-4a9d-8052-9d312405daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "info_new = restoreSeq(ord_info_output, r_idx)\n",
    "print(info_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90c17ba4-03cd-4ff0-83a7-876f899a9e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6, 4, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "output_size = dim\n",
    "info_output = info_new.view(*list(leng_mask.shape) + [output_size])\n",
    "print(info_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bf4fad8-a610-47a6-815d-bfd82c2e40bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8383, -0.9131,  0.5006,  0.0000],\n",
       "          [ 0.5006, -0.2046, -0.2352,  1.3841],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.1936, -0.0567,  0.5737, -0.4665],\n",
       "          [-0.7809, -0.4120,  0.0000,  0.0000],\n",
       "          [-0.2352,  1.1215,  0.0000,  0.0000],\n",
       "          [-1.1881,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.5259,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.2825, -1.1227, -2.8148,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.1142, -3.0648,  0.1113, -0.6002],\n",
       "          [-0.0567,  0.8383,  0.4693,  0.0000],\n",
       "          [-0.4740, -0.7515,  1.0453,  0.8504],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.2369, -0.7212,  0.0000,  0.0000],\n",
       "          [-0.9093,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.8121, -1.8616,  0.0000,  0.0000],\n",
       "          [-0.1936,  0.2140, -0.0567, -0.1639]],\n",
       "\n",
       "         [[-0.4665,  1.3841, -0.2369,  0.4495],\n",
       "          [ 0.8278, -0.0567,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-1.3453,  1.0453,  0.5006,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0339, -0.1733, -0.2481, -1.8616],\n",
       "          [-0.6543,  0.4705, -0.4120,  1.0334],\n",
       "          [-0.4007, -0.3524,  1.2555, -0.4740],\n",
       "          [-0.4413,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 1.1766,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.0567,  0.4693,  0.0000,  0.0000],\n",
       "          [-0.1783, -0.2481,  1.0184,  0.8099],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0543, -0.2481, -0.9516,  0.0000],\n",
       "          [-0.3592, -0.2576,  0.5737,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 1.1163, -0.0567,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 2.1961,  1.2555,  0.0000,  0.0000],\n",
       "          [ 1.3841,  1.1163,  0.0000,  0.0000],\n",
       "          [-0.5296,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.3592,  0.2825, -0.0567,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000]]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_output[:,:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17c872ad-09f1-4b92-af98-1bb8749e5e3e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[False, False, False,  True],\n",
       "          [False, False, False, False],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]],\n",
       "\n",
       "         [[False, False, False, False],\n",
       "          [False, False,  True,  True],\n",
       "          [False, False,  True,  True],\n",
       "          [False,  True,  True,  True]],\n",
       "\n",
       "         [[False,  True,  True,  True],\n",
       "          [False, False, False,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]],\n",
       "\n",
       "         [[False, False, False, False],\n",
       "          [False, False, False,  True],\n",
       "          [False, False, False, False],\n",
       "          [ True,  True,  True,  True]],\n",
       "\n",
       "         [[False, False,  True,  True],\n",
       "          [False,  True,  True,  True],\n",
       "          [False, False,  True,  True],\n",
       "          [False, False, False, False]],\n",
       "\n",
       "         [[False, False, False, False],\n",
       "          [False, False,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]],\n",
       "\n",
       "         [[False, False, False, False],\n",
       "          [False, False, False, False],\n",
       "          [False, False, False, False],\n",
       "          [False,  True,  True,  True]],\n",
       "\n",
       "         [[False,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]],\n",
       "\n",
       "         [[False, False,  True,  True],\n",
       "          [False, False, False, False],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False,  True],\n",
       "          [False, False, False,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]],\n",
       "\n",
       "         [[False, False,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]],\n",
       "\n",
       "         [[False, False,  True,  True],\n",
       "          [False, False,  True,  True],\n",
       "          [False,  True,  True,  True],\n",
       "          [False, False, False,  True]],\n",
       "\n",
       "         [[ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]],\n",
       "\n",
       "         [[ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leng_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbb706-d953-4244-ad79-a5a5670ab8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a158a-1c01-47f9-9a58-77092d451c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
